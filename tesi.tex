% draft 3rd year diploma thesis, by Nick Manini, 2013/03/22
\documentclass[a4paper,12pt]{article}
\usepackage[english]{babel} % or other languages, e.g:
%\usepackage[italian]{babel} % needs debian package texlive-lang-italian
%\usepackage[latin1]{inputenc} % use to reproduce accented characters correctly
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{enumitem}
\usepackage{framed}
\usepackage[width=125mm]{caption}

\graphicspath{{./media/images/}}

\usepackage[document]{ragged2e}
\usepackage{verbatim}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[LE,RO]{\slshape}
\fancyfoot[C]{\thepage}

%include the git footer with the last commit
\include{gitfooter}

% Dimensione della pagina
\setlength{\oddsidemargin}{.3in}  % Distance from the left edge -1 inch 
\setlength{\textwidth}{145mm}     % Normal width of the text
\setlength{\topmargin}{.25in}     % Distance from top to PAGE'S HEAD -1 inch
\setlength{\textheight}{225mm}    % Height of the body of page
\setlength{\headheight}{0mm}      % Height of a box containing the head
\setlength{\parskip}{0.5mm}         % Extra vertical space before a paragraph
\setlength{\parindent}{9mm}       % Width of the indentation 
\linespread{1.12}                 % Line spacing        
\renewcommand{\floatpagefraction}{.9}

\usepackage{xcolor}
\newcommand\mynotes[1]{\begin{flushright}

\textcolor{red}{TODO: #1}\end{flushright}}
\newcommand{\jsqrt}[2]{\bqty{ #1 #1 | #2 #2 }}
\newcommand{\ksqrt}[2]{\bqty{ #1 #2 | #2 #1 }}
\newcommand\mf[1]{\mathbf{#1}}
\newcommand\dens{\rho(\mathbf{r})}
\newcommand\erre{\mathbf{r}}

\begin{document}

\title{\bf \Huge Titolazzo della tesi, if seems long\\go to newline }


\author{Giorgio Ruffa\\
Dipartimento di Fisica, Universit\`a degli Studi di Milano,\\
Via Celoria 16, 20133 Milano, Italia
}
\date{April 22, 2016} % the exact date of graduation, when available


\include {frontespizio} % eccezionalmente qui include il frontespizio
% in general avoid \include altoghether, it is looking for trouble !!

\newpage\qquad
\newpage

\maketitle

%---------------------------------------------------------
\begin{abstract}

A long time ago in a galaxy far, far away...

\vskip0.75cm
\hskip5cm
\parbox[t]{7cm}
{
Advisor: {\it Dott. Dario Tamascelli}\\
Co-Advisor: {\it Prof. Michele Ceotto}\\
External Advisor: {\it Dott. Davide Ceresoli}\\
}
\end{abstract}
%--------------------------------------------------------

\newpage
\tableofcontents
\newpage


\section{Ab Initio calculations in modern Solid State Physic}
\mynotes{importanza dei conti nel calcula delle strutture elettroniche}


\section{Theoretical Introduction}\label{model:sec}

This section will cover the basic theory upon which Quantum ESPRESSO is based.

By starting with a very general approach we can say that the Hamiltonian associated to a system of atoms with $N_N$ nuclei and $N_e$ electrons can be written as:


\begin{equation}\label{eq:theHamiltonianLong}
\hat{H}_{tot} = \hat{T}_{N} + \hat{T}_{e} + \hat{V}_{Ne} + \hat{V}_{NN} + \hat{V}_{ee}
\end{equation}

Where:

\begin{equation}
\hat{T}_{N} = - \frac{\hbar}{2} \sum_{\alpha}^{N_N} \frac{\nabla_{\alpha}^2}{M_{\alpha}}
\end{equation}
is the kinetic energy of the nuclei

\begin{equation}
\hat{T}_{e} = - \frac{\hbar}{2m_{e}} \sum_{i}^{N_e} \nabla_{i}^2
\end{equation}
is the kinetic energy of the electrons

\begin{equation}
\hat{V}_{Ne} = -\frac{e^2}{4\pi\varepsilon_{0}} \sum_{i}^{N_e}\sum_{\alpha}^{N_N} \frac{Z_{\alpha}}{\mid R_{\alpha} - r_{i} \mid }
\end{equation}
is the electron-ion attraction potential energy

\begin{equation}
\hat{V}_{NN} = \frac{e^2}{4\pi\varepsilon_{0}} \frac{1}{2} \sum_{\alpha \neq \beta}^{N_N} \frac{Z_{\alpha} Z_{\beta}}{\mid R_{\alpha} - R_{\beta} \mid }
\end{equation}
is the nucleus-nucleus repulsion potential energy

\begin{equation}
\hat{V}_{ee} = \frac{e^2}{4\pi\varepsilon_{0}} \frac{1}{2} \sum_{i \neq j}^{N_e} \frac{1}{\mid r_{i} - r_{j} \mid }
\end{equation}
is the electron-electron repulsion potential energy

A state function $\ket{\psi}$ describing all the particles involved in the system will evolve following the Schr\"odinger equation.
\begin{equation}\label{eq:eq_sch}
	i\hbar\dv{t}\ket{\psi(t)} = \hat{H}_{tot}\ket{\psi(t)}
\end{equation}

From now on, if not explicitly specified, we will use atomic units \footnote{see \cite[p.42]{Attila} for further details}
\begin{equation}
	m_{e} = \hbar = e =\frac{1}{4 \pi \epsilon_{0}} = 1
\end{equation}

we shall rewrite equation \eqref{eq:theHamiltonianLong} in a much more elegant form:


\begin{equation}\label{eq:theHamiltonian}
\boxed{
	\hat{H}_{tot}   = - \sum_{\alpha}^{N_N} \frac{\nabla^2_{\alpha}}{2M_{\alpha}}
					+ \frac{1}{2}\sum_{\alpha \neq \beta}^{N_N} \frac{Z_{\alpha} Z_{\beta}} {R_{\alpha \beta}}
					- \sum_{i}^{N_e} \frac{\nabla_{i}^2}{2}
					- \sum_{i}^{N_e} \sum_{\alpha}^{N_N} \frac{ Z_{\alpha} }{r_{i \alpha}}				
					+ \frac{1}{2} \sum_{i \neq j}^{N_e} \frac{1}{r_{ij}}
}
\end{equation}
With:
\begin{align*}
	r_{ij} & = \mid r_{i} - r_{j} \mid 
\\
	r_{i \alpha} & = \mid r_{i} - R_{\alpha} \mid 
\\
	R_{\alpha \beta} & = \mid R_{\alpha} - R_{\beta} \mid 
\end{align*}

Although the universality of this equation\footnote{It must be noted that we are neglecting any relativistic effect.}, we know very well that even a simple molecule like $H_2^{+}$ has no analytical solution.

Thus, even from a computational standpoint, a set of approximations must be performed.


\subsection{The Born-Oppenheimer Approximation}

The Born-Oppenheimer approximation takes note of the great difference in masses of electrons and nuclei.
Nuclear mass is much higher than electron mass, so we expect electrons to have much higher velocities than	 nuclei. 

It's now reasonable to separate the motion of the system in two distinct movements: the \textit{``slow"} movement of the nuclei, and the \textit{``fast"} movement of the electrons.

We can say that from the point of view of electrons, the nuclei appear to be fixed. 
From a physical standpoint the electrons are moving in the static field produced by the nuclei while still interacting within each others \cite[p.241]{Atkins}.


Using this approximation the kinetic energy of the nuclei $\hat{T}_{NN}$ can be neglected and the repulsion between the nuclei $\hat{V}_{NN}$, can be considered to be constant.

We will consider now the electronic Hamiltonian $H_{elec}$ :
\begin{equation}
	\hat{H}_{e} = \hat{T}_{e} + \hat{V}_{Ne} + \hat{V}_{ee}
\end{equation}

Rewriting $\hat{H}_{e}$  using atomic units:
\begin{equation}\label{eq:H_elec}
	\hat{H}_{e} = 	- \sum_{i}^{N_{e}} \frac{1}{2} \nabla_{i}^2  
					- \sum_{i}^{N_{e}} \sum_{\alpha}^{N_{N}} \frac{Z_{\alpha}}{r_{i\alpha}}  
					+ \frac{1}{2} \sum_{i \neq j}^{N_{e}} \frac{1}{r_{ij}}
\end{equation}

Then equation \eqref{eq:eq_sch} then becomes:
\begin{equation}
	\hat{H}_{e} \ket{\Phi_{e}} = \varepsilon_{e} \ket{\Phi_{e}}
\end{equation}

With solution
\begin{equation}
	\ket{\Phi_{e}} = \ket{\Phi_{e}( \{r_{i}\};\{R_{\alpha}\} )}
\end{equation}

Where the dependency from the electronic coordinates $\{r_i\}$ is explicit, but the dependency from nuclear coordinates $\{R_{\alpha}\}$ is parametric.
This implies that also the electronic energy depends parametrically on $\{R_{\alpha}\}$

\begin{equation}
	\varepsilon_{e} = \varepsilon_{e}(\{R_{\alpha}\})
\end{equation}

To obtain the total energy (with fixed nuclei) we have to add the constant ion-ion Coulomb potential energy

\begin{equation}\label{eq:totEn1}
	\varepsilon_{tot} = \varepsilon_{e} + \frac{1}{2} \sum_{\alpha \neq \beta}^{N_N} \frac{Z_{\alpha} Z_{\beta} }{R_{\alpha \beta}}
\end{equation}

Equation from \eqref{eq:H_elec} to \eqref{eq:totEn1} constitutes the so called \textit{``Electronic Problem"} \cite[p.44]{Attila}.

Once solved one could then apply the same principle to the nuclear problem.
Since the electrons moves much faster then the nuclei, it is reasonable to approximate \eqref{eq:theHamiltonian} by replacing electronic coordinates by their average value, averaged over $\Phi_{e}$.
The nuclear Hamiltonian will be :
\begin{equation}
	H_{N} = - \sum_{\alpha}^{N_{\alpha}} \frac{1}{2M_{\alpha}} \nabla_{\alpha}^2 + \varepsilon_{tot}(\{ R_{\alpha}\})
\end{equation}

We can see that $\varepsilon_{tot}(\{ R_{\alpha}\})$ is a potential energy surface for nuclear motion.

Thus the nuclei in the Born-Oppenheimer approximation move on a potential energy surface obtained by solving the electronic problem.

The nuclear Schr\"odinger equation  
\begin{equation}
	\hat{H}_{N} \ket{\Phi_{N}} = \varepsilon \ket{\Phi_{N} }
\end{equation}

with solution 

\begin{equation}
	\ket{\Phi_{N}}=\ket{\Phi_{N}(\{R_{\alpha}\})}
\end{equation}

will describe vibration, rotation, and translation of the molecule.

The complete approximate solution to \eqref{eq:theHamiltonian} will be  \cite[p.43-45]{Attila}

\begin{equation}
	\ket{ \Phi(\{r_i\};\{R_{\alpha}\}) } 	= \ket{ \Phi_{e}(\{r_i\};\{R_{\alpha}\})} 
											~ \ket{ \Phi_{N}(\{R_{\alpha}\})}
\end{equation}

Since this work will mainly focus on the \textit{``electronic problem"}, from now on we will drop the suffix ``$e$" for the Hamiltonian $H_{e}$.

\subsection{The Hartree Product}
\subsubsection{Spin-Orbit}
We introduce two single particle spin functions  $\alpha(\omega), \beta(\omega)$, corresponding respectively to spin up and spin down.
The only conditions we impose is that these two functions are orthonormal and form a complete set of auto-functions for the spin operator $\hat{S}_z$ \footnote{Even if it isn't necessary to specify the eigenvalues of the spin operator (as stated in \cite{Attila}), we prefer to specify them since we'll deal only with fermions. }.

\begin{align*}
	\bra{\alpha}\ket{\alpha} & = \bra{\beta}\ket{\beta} = 1 \\
	\bra{\alpha}\ket{\beta} & = \bra{\beta}\ket{\alpha} = 0 \\
	\hat{S}_{z} \ket{\alpha} & = + \frac{1}{2} \ket{\alpha} \\
	\hat{S}_{z} \ket{\beta} & = - \frac{1}{2} \ket{\beta} \\
\end{align*}

An electron is described by three spacial coordinates $\mathbf{r}$ and one spin coordinate $\omega$.
\begin{equation}
	\mathbf{x} = \{\mathbf{r},\omega\}
\end{equation}

We define $\psi_i(\mathbf{r})$ a spacial orbital as a function of the position vector $\mathbf{r}$ describing the spacial distribution of an electron, so that $\mid\psi_i(\mathbf{r})\mid^2 {dr}^3$ is the probability of finding the electron in a small volume ${dr}^3$ centered at position $\mathbf{r}$.

Given a set of spatial orbitals we will assume them to be orthonormal, thus if the set is complete we can express any spatial state function as a linear combination of spatial orbitals.

In general, for the set to be complete, one should consider it as infinite. As one can imagine, in practice, an infinite orbitals' set is not available and we will consider only a finite set composed of $K$ such orbitals. The finite set will only cover a certain region of the complete space, but we will consider the results to be ``exact" within the subspace generated by the finite set of spatial orbitals.

The combination of a spatial orbital and a spin function is called a spin orbital and completely describes the electron. 
\begin{equation}
	\chi_{i}(\mathbf{x}) = \psi_i(\mathbf{r}) \alpha(\omega)
\end{equation}


For a set of K spatial orbital will have a corresponding set of $2K$ spin orbitals, each pair sharing the same spatial orbital but with different spin function:

\begin{equation}
	\chi_{2i} = \psi_i(\mathbf{r}) \alpha(\omega)
\end{equation}
\begin{equation}
	\chi_{2i-1} = \psi_i(\mathbf{r}) \beta(\omega)
\end{equation}

Being the spatial orbital and spin orbitals orthonormal, so are the spin orbitals.

\subsubsection{Separating the problem}

It is evident that the term $V_{ee}$ in \eqref{eq:H_elec} is the more complicated to handle in order to find an exact solution to the electronic problem.
To simplify the system we start by neglecting $V_{ee}$, considering the electrons as non interacting.
The Hamiltonian \eqref{eq:H_elec} can be rewritten in the following form:
\begin{align}
	\hat{H} & = \sum_{i}^{N_{e}} \hat{h}_{i} \label{eq:HartreeHamiltonian} 
	\\
	\hat{h}_{i} & = \frac{1}{2} \nabla_{i}^2 - \sum_{\alpha}^{N_{N}} \frac{Z_{\alpha}}{r_{i\alpha}}  \label{eq:singleElHam}
\end{align}


In this form $\hat{H}$ is composed by a sum of $N_e$ independent single electron Hamiltonians of the form \eqref{eq:singleEl}.

Each operator $\hat{h}_i$ will have its spin orbital eigenfunction with eigenvalue given by
\begin{equation}
	\hat{h}_{i} \ket{\chi_{i}(\mathbf{x}_i) } = \varepsilon_{i} \ket{\chi_{i}(\mathbf{x}_i) }
\end{equation}

Since \eqref{eq:HartreeHamiltonian} is a sum of independent Hamiltonians his eigenfunctions will be the tensor product of $\hat{h}_i$ eigenfunctions
\begin{align}\label{eq:HartreeProduct}
	\ket{\Psi^{HP}(\mathbf{x}_{1},...,\mathbf{x}_{N_e})} = & \ket{\chi_1(\mathbf{x}_{1})} \otimes \cdots  \otimes \ket{\chi_N(\mathbf{x}_{N})} \\
	:= & \ket{ \chi_{1}(\mathbf{x}_{1}) \cdots \chi_{N}(\mathbf{x}_{N}) }
\end{align}
with eigenvalue
\begin{equation}
	\hat{H}\ket{\Psi^{HP}} = E\ket{\Psi^{HP}}
\end{equation}
\begin{equation}
	E = \varepsilon_i + \varepsilon_j + ... + \varepsilon_k
\end{equation}

It must be noted that in equation \eqref{eq:HartreeProduct} there is no correlation between the index of the spin-orbital $\chi_{i}$ to the index of the electron's coordinates $\mathbf{x}_i$. It is absolutely and completely reasonable to have the $j$-th electron in the $i$-th spin-orbital e.g : $\chi_{j}(\mathbf{x}_{i})$.
One should also consider that the number of spin-orbitals that describes our system can be (and often is) greater than the total number of electrons. The use of the same index is, by any means, an excess of notation.


Equation \eqref{eq:HartreeProduct} is called an \textit{Hartree Product} and has the following straightforward property: 
\begin{equation}\label{eq:uncorrelated}
	\mid\Psi^{HP}(\mathbf{x}_1,\cdots,\mathbf{x}_{N_e}) \mid^2 dx_1^3 \cdots dx_{N_e}^3 = \mid\chi_i(\mathbf{x}_1)\mid^2 dx_1^3 \cdots \mid \chi_k(\mathbf{x}_{N_e})\mid^2 dx_{N_e}^2
\end{equation}

Meaning that the probability of finding simultaneously (with an unique measure) each electron in a fixed position (within a small volume) is equal to the uncorrelated probability of finding electron 1 in position $x_1$ times the probability of electron 2 in position $x_2$ and so on (by independent measurements).
For this reason $\Psi^{HP}$ is called an uncorrelated or electron-independent wave function. 
Namely, the position of one electron has no effect on the position of the others. 

This is, of course, a strong assumption, but we will see how the Hartree-Fock method will correct this approximation.



\subsubsection{Identical Particles and Slater Determinant}

$\Psi^{HP}$ is the simplest representation of a multi-electron state function, but it does not respect the invariance by exchange of identical particles.
Since elementary particles like electrons are identical or indistinguishable, the probability distribution associated to the state function describing the entire system should remain the same if the coordinates of two or more particles are exchanged.

\begin{equation}
	\mid \Psi(\mathbf{x}_1,\dots,\mathbf{x}_i,\dots,\mathbf{x}_j,\dots,\mathbf{x}_{N_e}) \mid^2 = \mid \Psi(\mathbf{x}_1,\dots,\mathbf{x}_j,\dots,\mathbf{x}_i,\dots,\mathbf{x}_{N_e}) \mid^2
\end{equation}

This implies one of the following conditions:
\begin{align}
	\Psi(\mathbf{x}_1,\dots,\mathbf{x}_i,\dots,\mathbf{x}_j,\dots,\mathbf{x}_{N_e}) = 
		+\Psi(\mathbf{x}_1,\dots,\mathbf{x}_j,\dots,\mathbf{x}_i,\dots,\mathbf{x}_{N_e})\\
	\Psi(\mathbf{x}_1,\dots,\mathbf{x}_i,\dots,\mathbf{x}_j,\dots,\mathbf{x}_{N_e}) = 
		-\Psi(\mathbf{x}_1,\dots,\mathbf{x}_j,\dots,\mathbf{x}_i,\dots,\mathbf{x}_{N_e}) 
\end{align}

Namely that the state function is respectively symmetric or antisymmetric.
Particles with integer spin, like protons, are called bosons and are always represented by symmetric wave functions.
Particles with half integer spin, like electrons, are called fermions and are always represented by antisymmetric wave functions.

We can immediately verify that even the simplest Hartree product composed by only two spin orbit does not respect the antisymmetry principle.

\begin{equation*}
	\ket{\Psi^{HP}(\mathbf{x}_1,\mathbf{x}_2)} = \ket{\chi_1(\mathbf{x}_1)} \ket{\chi_2(\mathbf{x}_2)} \neq \ket{\chi_1(\mathbf{x}_2)} \ket{\chi_2(\mathbf{x}_1)}
\end{equation*}

The easiest way to make $\Psi^{HP}$ antisymmetric is to introduce what is called an \textit{exchange term}.

\begin{equation}\label{eq:slater2}
	\ket{\Psi(\mathbf{x}_1,\mathbf{x}_2)} = \frac{1}{\sqrt{2}} (\ket{\chi_1(\mathbf{x}_1)} \ket{ \chi_2(\mathbf{x}_2)} - \ket{\chi_1(\mathbf{x}_2)} \ket{\chi_2(\mathbf{x}_1)} )
\end{equation}

We can rewrite \eqref{eq:slater2} as the determinant of the matrix :
\begin{equation}
\ket{\Psi(\mathbf{x}_1,\mathbf{x}_2)} = \frac{1}{\sqrt{2}}
\begin{vmatrix}
\chi_1(\mathbf{x}_1) & \chi_2(\mathbf{x}_1) \\
\chi_1(\mathbf{x}_2) & \chi_2(\mathbf{x}_2) 
\end{vmatrix}
\end{equation}

The generalization to $N_e$ electrons held to:

\begin{equation}\label{eq:SlaterDet}
\ket{\Psi(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N)} =
\frac{1}{\sqrt{N!}}
\left|
	\begin{matrix} 
   		\chi_1(\mathbf{x}_1) & \chi_2(\mathbf{x}_1) & \cdots & \chi_N(\mathbf{x}_1) \\
        \chi_1(\mathbf{x}_2) & \chi_2(\mathbf{x}_2) & \cdots & \chi_N(\mathbf{x}_2) \\
		\vdots & \vdots & \ddots & \vdots \\
        \chi_1(\mathbf{x}_N) & \chi_2(\mathbf{x}_N) & \cdots & \chi_N(\mathbf{x}_N)
    \end{matrix} 
	\right|\equiv \left| 
	\begin{matrix}
		   \chi _1 & \chi _2 & \cdots  & \chi _N  \\
	\end{matrix}
   \right|,
\end{equation}

Equation \eqref{eq:SlaterDet} is called the \textit{Slater Determinant} and is the simplest asymmetrical representation of a multi-electron state function.

Using a more formal approach outlined in \cite[p.357-362]{Sakurai} we can express the exchange of two coordinates in \eqref{eq:HartreeProduct} as the action of the transposition operator $\hat{P}_{ij}$ 
\begin{align} \label{eq:transpositionOp}
	\begin{split}
		\hat{P_{ij}} & \ket{\chi_1(\mathbf{x}_1)   \cdots  \chi_i(\mathbf{x}_i)  \cdots  \chi_j(\mathbf{x}_j)  \cdots   \chi_N(\mathbf{x}_N)} =   \\ 
		& \ket{\chi_1(\mathbf{x}_1)   \cdots  \chi_j(\mathbf{x}_i)  \cdots  \chi_i(\mathbf{x}_j) \cdots  \chi_N(\mathbf{x}_N)} 
	\end{split}
\end{align}
\begin{equation*}
	\hat{P}_{ij} = \hat{P}_{ji} ~;~
	\hat{P}_{ij}^2 = \mathbf{1}
\end{equation*}

Note that in equation \eqref{eq:transpositionOp} we maintained the position of the electron index and switched the spin-orbital index. In this way it is possible to rewrite \eqref{eq:transpositionOp} in a more compact way :
\begin{equation}
	\hat{P_{ij}}  \ket{\chi_1  \cdots  \chi_j  \cdots  \chi_i  \cdots   \chi_N} =  
		 \ket{\chi_1   \cdots  \chi_j  \cdots  \chi_i \cdots  \chi_N} 
\end{equation}
The sequential position of the spin-orbitals indicates coincides with the index of the electron represented \cite{Sakurai}.


For the ket state to be antisymmetric it must be an eigenfunction of $\hat{P}_{ij}$ with eigenvalue $-1$.
\begin{align*}
	\hat{P_{ij}} & \ket{\chi_1(\mathbf{x}_1)   \cdots  \chi_i(\mathbf{x}_i)  \cdots  \chi_i(\mathbf{x}_j)  \cdots   \chi_N(\mathbf{x}_N)} =  \\
	-1 & \ket{\chi_1(\mathbf{x}_1)   \cdots  \chi_i(\mathbf{x}_i)  \cdots  \chi_i(\mathbf{x}_j) \cdots  \chi_N(\mathbf{x}_N)} 
\end{align*}

It can be shown that an arbitrary permutation of N objects can be written as a product of transpositions and that the number of transposition in this decomposition is of fixed parity. That is, either a permutation is always decomposed in an even number of transpositions (the permutation is called even and has the parity $+1$), or a permutation is always decomposed in an odd number of transpositions and then it is an odd permutation with parity  $ - 1$.

Denoting the parity of the arbitrary permutation as $\sigma_P$ it follows that antisymmetric function must respect the following condition :
\begin{align*}
	\hat{P} & \ket{\chi_1(\mathbf{x}_1)   \cdots  \chi_i(\mathbf{x}_i)  \cdots  \chi_i(\mathbf{x}_j)  \cdots   \chi_N(\mathbf{x}_N)} = \\
	(-1)^{\sigma_P}  & \ket{\chi_1(\mathbf{x}_1)   \cdots  \chi_i(\mathbf{x}_i)  \cdots  \chi_i(\mathbf{x}_j)  \cdots   \chi_N(\mathbf{x}_N)}
\end{align*}

If $S_N$ is the group of all possible $N!$ permutations we can define the \textit{antisymmetrizer} operator as :
\begin{equation}\label{eq:antisymmetrizer}
	\mathcal{A} = \frac{1}{N!} \sum_{\hat{P} \in S_n} (-1)^{\sigma_P} \hat{P}
\end{equation}

We can now re-express the Slater determinant \eqref{eq:SlaterDet} in a more useful form. Using the Leibniz formula for determinants we rewrite the determinant as:
\begin{equation}\label{eq:SlaterLeibniz}
	\left|
	\begin{matrix}
		   \chi _1 & \chi _2 & \cdots  & \chi _N  \\
	\end{matrix} 
	\right| = \frac{1}{\sqrt{N!}} \sum_{\hat{P} \in S_n} (-1)^{\sigma_P} \hat{P} \ket{\chi_1(\mathbf{x}_1) \cdots   \chi_N(\mathbf{x}_N)}
\end{equation}
Thus:
\begin{equation}
	\left|
	\begin{matrix}
		   \chi _1 & \chi _2 & \cdots  & \chi _N  \\
	\end{matrix} 
	\right| =  (\sqrt{N!}) \mathcal{A} \ket{ \chi_1(\mathbf{x}_1) \cdots   \chi_N(\mathbf{x}_N) }
\end{equation}

This formalism will be useful when dealing with one-electron and two-electrons operators.

\subsubsection{Properties of the Slater determinant}\label{sec:slaterPropr}

It is immediate to verify that by exchanging two electrons in \eqref{eq:SlaterDet} the sign of the determinant changes by a factor of $- 1$, respecting the antisymmetry principle.

Another interesting property is that if two different electrons occupy completely the same spin-orbitals, the matrix will have two identical rows (because the electrons are identical the electron index is to be considered mute) and the determinant will be null. So the Slater determinant respects the Pauli principle, two identical fermions cannot occupy the same spacial orbital having both the same spin (i.e. cannot be described by the same quantum numbers) \footnote{A different formulation of the Pauli principle is that a wave function of identical fermions must be an eigenfunction of a transposition operator with its parity as eigenvalue}.


To see the effects of the anti-simmetrisation requirement we now consider a system made of two particles. We pick a starting Hartree product of the types: 
\begin{align*}
	\chi_{1}(\mathbf{x}_{1}) = \psi_{1}(\mathbf{r}_{1}) \alpha(\omega_{1})\\
	\chi_{2}(\mathbf{x}_{2}) = \psi_{2}(\mathbf{r}_{1}) \beta(\omega_{2})
\end{align*}
where the two particles occupies two different orbitals each with different spin.

If we anti-symmetrize the product (using the associated Slater determinant), the probability $P(\mathbf{r}_{1},\mathbf{r}_{2}) d\mathbf{r_{1}} d\mathbf{r_{2}}$ of finding electron 1 in position $\mathbf{r}_{1}$ and electron 2 in position $\mathbf{r}_{2}$ within a small volume is equal to \cite[p.52]{Attila}:
\begin{equation*}
	P(\mathbf{r}_{1},\mathbf{r}_{2}) = 
		\frac{1}{2} ( 
			\mid \psi_{1}(\mathbf{r}_1) \mid ^2    
			\mid \psi_{2}(\mathbf{r}_2) \mid ^2   
				+
			\mid \psi_{1}(\mathbf{r}_2) \mid ^2    
			\mid \psi_{2}(\mathbf{r}_1) \mid ^2   
			)
\end{equation*}
In this case the probability is the average of the two possible configuration: electron 1 in $\psi_1$ and electron 2 in $\psi_2$; electron 1 in $\psi_2$ and electron 2 in $\psi_1$.
In this case the probabilities are said to be \textit{uncorrelated}.

But what happens if we try to exchange two particles with the same spin but different orbitals?
\begin{align*}
	\chi_{1}(\mathbf{x}_{1}) = \psi_{1}(\mathbf{r}_{1}) \beta(\omega_{1})\\
	\chi_{2}(\mathbf{x}_{2}) = \psi_{2}(\mathbf{r}_{1}) \beta(\omega_{2})
\end{align*}

What we obtain is \cite[p.53]{Attila}:
\begin{align*}
	P(\mathbf{r}_{1},\mathbf{r}_{2}) = 
		\frac{1}{2} [ &
			\mid \psi_{1}(\mathbf{r}_1) \mid ^2    
			\mid \psi_{2}(\mathbf{r}_2) \mid ^2   
				+
			\mid \psi_{1}(\mathbf{r}_2) \mid ^2    
			\mid \psi_{2}(\mathbf{r}_1) \mid ^2   
\\
			& - ( \psi_{1}^*(\mathbf{r}_1) \psi_{2}(\mathbf{r}_1) \psi_{2}^*(\mathbf{r}_2) \psi_{1}(\mathbf{r}_2)
			 + \psi_{1}(\mathbf{r}_1) \psi_{2}^*(\mathbf{r}_1) \psi_{2}(\mathbf{r}_2) \psi_{1}^*(\mathbf{r}_2))]
\end{align*}
where we obtained an extra cross term that makes the probabilities \textit{correlated}. Note that thanks to the extra term we have that $P(\mathbf{r}_{1},\mathbf{r}_{1}) = 0$, respecting the Pauli principle.

This two situations are called respectively a \textit{Fermi Heap} and a \textit{Fermi Hole} \cite{Dan}.


\subsection{The Hartree-Fock method} \label{sec:HF}
We have seen that thanks to the Born-Oppenheimer approximation it is possible to consider as separated the motion of nuclei and the motion of electrons. 
Focusing on the electronic problem we show that, by neglecting $\hat{V}_{ee}$, it is possible to separate the Hamiltonian \eqref{eq:H_elec} into $N_e$ independent Hamiltonians \eqref{eq:HartreeHamiltonian}.
What the Hartree-Fock method aims to do is to take under consideration the interaction between electrons while keeping the advantage of a separable Hamiltonian.

\subsubsection{One-electron and Two-electrons Operators}

Using  equation \eqref{eq:singleElHam} we rewrite equation \eqref{eq:H_elec} in his complete form \footnote{Where we have dropped the \textit{e} index}
\begin{equation}
	\hat{H} = \sum_{i}^{N} \hat{h}_{i} + \frac{1}{2} \sum_{i \neq j}^{N} \frac{1}{r_{ij}}
\end{equation}

The operator $\sum_{i}^{N} \hat{h}_{i}$ is called a \textit{one-electron operator} with the following formal definition:

\begin{equation}\label{eq:singleEl}
	\mathcal{O}_{1} := \sum_{i}^{N} \hat{h}_{i}
\end{equation}

Similarly, operator $\frac{1}{2} \sum_{i \neq j}^{N} \frac{1}{r_{ij}}$ is called a \textit{two-electron operator}:

\begin{equation}
	\mathcal{O}_{2} := \frac{1}{2} \sum_{i \neq j}^{N} \frac{1}{r_{ij}}
\end{equation}

We are interested in the effect these two operators have when applied on a single Slater determinant.

Using \eqref{eq:SlaterLeibniz}, the orthonormality of the spin-orbitals composing the Slater determinant \eqref{eq:SlaterDet} and the fact that $r_{ij} = r_{ji}$ one can show that \cite[p.74-81]{Attila}:

\begin{align}
	& \bra{\Psi} \mathcal{O}_{1} \ket{\Psi} = \sum_{i}^{N} \bra{\chi_{i}} \hat{h}_i \ket{\chi_{i}} \\
	& \bra{\Psi} \mathcal{O}_{2} \ket{\Psi} = \frac{1}{2} \sum_{i \neq j}^{N} \bra{\chi_i \chi_i} \frac{1}{r_{ij}} \ket{ \chi_{i} \chi_{i}} - \bra{\chi_i \chi_j} \frac{1}{r_{ij}} \ket{ \chi_{j} \chi_{i}}
\end{align}

Introducing the following notation:
\begin{align}
	\bra{\chi_{i}} \hat{h}_i \ket{\chi_{i}} = \bra{i} \hat{h} \ket{i}
	\\
	\bra{\chi_{i} \chi_{j}} \frac{1}{r_{ij}} \ket{ \chi_{j} \chi_{i}} = \jsqrt{i}{j} \label{eq:squareNotation}
\end{align}

we can express expectation energy for a single Slater determinant as :
\begin{equation}
	\bra{\Psi} \hat{H} \ket{\Psi} = \sum_{i}^{N} \bra{i} \hat{h} \ket{i} + \frac{1}{2} \sum_{i \neq j}^{N} \jsqrt{i}{i} - \ksqrt{i}{j}
\end{equation}

Still the equation is not separable and far from being reduced to an eigenvalue problem, but we have an handy expression for the expectation energy of the state of the system

\subsubsection{Variational Method}
So far we made no consideration on the spin-orbitals $\{ \chi_{i} \}$ composing \eqref{eq:SlaterDet}, besides the orthonormality condition.

But what if we want to pick the $\{ \chi_{i} \}$ that correspond to the ground state of our system?

The variational principle comes in help stating that the exact ground state solution to the Schr\"odinger equation $\hat{H} \ket{\Psi} = E \ket{\Psi}$ is the one that minimizes the functional 
\begin{equation} \label{eq:hamFunctional}
	E_{0}[\{\chi_{i}\}] = \bra{\chi_1 \cdots \chi_N} \hat{H} \ket{\chi_1 \cdots \chi_N} = \bra{\Psi} \hat{H} \ket{\Psi}
\end{equation}

To find the minimum of the functional\eqref{eq:hamFunctional} we must consider his differential for a small variation $\var{\Psi}$ of the test solution $\Psi$  \cite[p.165]{Carati}.  

\begin{align}
	\ket{\Psi} & \rightarrow \ket{\Psi + \var{\Psi}} 
\\
	\bra{\Psi + \var{\Psi}} \hat{H} \ket{\Psi + \var{\Psi}} & = 
		\bra{\Psi} \hat{H} \ket{\Psi} 
		+ \bra{\var{\Psi}} \hat{H} \ket{\Psi} 
		+ \bra{\Psi} \hat{H} \ket{\var{\Psi}} 
		+ \cdots
\\
		 & = E_{0}[\{\chi_{i}\}] + \var{E} + \cdots
\end{align}
Where
\begin{align}
	E_{0}[\{\chi_{i}\}] & = 	\bra{\Psi} \hat{H} \ket{\Psi}  \\
	\var{E} & = \bra{\var{\Psi}} \hat{H} \ket{\Psi} 	+ \bra{\Psi} \hat{H} \ket{\var{\Psi}} 
\end{align}
$\var{E}$ is the first order differential of the functional \eqref{eq:hamFunctional} \footnote{$\var{E}$ is not a real function differential, but a functional differential. Namely the part of the increment linear in $\var{\Psi}$.}.

We are looking for the set $\{\chi_i\}$ for which $\var{E} = 0$ \footnote{note that $\Psi$ is a Slater determinant $\mid \chi_1 \cdots \chi_i \cdots \chi_N \mid$}, but we want them to be orthonormal.

To satisfy both requirements we minimize the following functional \footnote{$\delta_{ij}$ is the Kronecker delta} 
\begin{align}
	\mathcal{L}[\{\chi_{i}\}] & = E[\{\chi_i\}] - \sum_{ij}^{N} \varepsilon_{ij} (\bra{\chi_{i}}\ket{\chi_{j}} - \delta_{ij})
\\
	\var{\mathcal{L}} & = \var{E} - \sum_{ij}^{N} \varepsilon_{ij} \var{(\bra{\chi_{i}}\ket{\chi_{j}})}	
\end{align}

If $\{\chi_i\}$ are orthonormal $\mathcal{L}[\{\chi_{i}\}]$ and $E[\{\chi_i\}]$ will have the minimum in the same \textit{``point"}. The elements $\varepsilon_{ij}$ are called the \textit{Lagrange multipliers}, and they form an Hermitian matrix $\{ \varepsilon \}_{ij}$.

Using the notation introduced in \eqref{eq:squareNotation}:

\begin{align}
	\var{E} 	& = \sum_{i}^{N} \bra{\var{i}} \hat{h}  \ket{i} + \bra{i} \hat{h}  \ket{\var{i}} \\
			& + \frac{1}{2} \sum_{ij}^{N} [ (\var{i})i \mid j j ] + [ i(\var{i}) \mid j j ] + [ i i \mid (\var{j}) j ] + [ i i \mid j (\var{j}) ]\\
			& - \frac{1}{2} \sum_{ij}^{N} [ (\var{i})j \mid j i ] + [ i(\var{j}) \mid j i ] + [ i j \mid (\var{i}) j ] + [ i j \mid j (\var{i}) ]
\end{align}

by splitting the sum and inverting the indexes we have \cite[p.117-119]{Attila}:
\begin{align}
	\begin{split}
	\var{E} = &  \sum_{i}^{N} \bra{\var{i}} \hat{h}  \ket{i} + \sum_{ij}^{N}  [ (\var{i})i \mid j j ] - [ (\var{i}) j \mid j i ]  \label{eq:trick} \\
	& +  cc
	\end{split}
\end{align}
where $cc$ is the complex conjugate of \eqref{eq:trick}.

Also
\begin{align}
\begin{split}
	\sum_{ij}^{N} \varepsilon_{ij} \var{(\bra{i}\ket{j})}  = & \sum_{ij}^{N} \varepsilon_{ij} \bra{\var{i}}\ket{j}  \label{eq:trick2}
	\\ & + cc
\end{split}
\end{align}

Putting together \eqref{eq:trick} and \eqref{eq:trick2} \footnote{ we are considering the complex conjugate.}:
\begin{align}
	\var{\mathcal{L}} & = \sum_{i}^{N} \bra{\var{i}} \hat{h}  \ket{i} + \sum_{ij}^{N}  [ (\var{i})i \mid j j ] - [ (\var{i}) j \mid j i ] + \sum_{ij}^{N} \varepsilon_{ij} \bra{\var{i}}\ket{j}\\
	& = \sum_{i}^{N} \bra{\var{i}} \left( 
		 \hat{h}  \ket{i} + \sum_{j}^{N}  [ i \mid j j ] - [  j \mid j i ] + \sum_{j}^{N} \varepsilon_{ij} \ket{j}
	\right) \label{eq:toNull}
	\\
	& = 0
\end{align}

since \eqref{eq:toNull} must be zero for every $i$ and for every possible variation $\var{i}$, the value between braces must  always be zero

\begin{equation}
	\hat{h}  \ket{i} + \sum_{j}^{N}  \bra{i} \frac{1}{r_{ij}} \ket{j j } - \bra{ j } \frac{1}{r_{ij}} \ket{j i} + \sum_{j}^{N} \varepsilon_{ij} \ket{j} = 0
\end{equation}

We define respectively the \textit{Coulomb operator} and the \textit{Exchanges operator} by:
\begin{align}
	J_{j}(1) \chi_{i}(1) = \left[  \int \dd \mathbf{x}_{2} \chi_{j}^{*}(2) \frac{1}{r_{12}} \chi_{j}(2) \right] \chi_i(1) \label{eq:coulombOperator} \\
	K_{j}(1) \chi_{i}(1) = \left[  \int \dd \mathbf{x}_{2} \chi_{j}^{*}(2) \frac{1}{r_{12}} \chi_{i}(2) \right] \chi_j(1)	
\label{eq:exchangeOperator}
\end{align}

where ``$(1)$" is the index of the integration variable of the function.

For every $i$, we finally have that :
\begin{align}
	\left[ \hat{h}(1) + \sum_{j}^{N} \hat{J}_{j}(1) - \hat{K}_{j}(1) \right] \ket{\chi_i(1)} = \sum_{j}^{N} \varepsilon_{ij} \ket{\chi_{j}(1)} \label{eq:HartreeNonCan}
\end{align}

We define the \textit{Fock operator}\footnote{note that for $i=j$ the term is null}
\begin{equation}\label{eq:FockOperator}
	\hat{f}(1) = \hat{h}(1) + \sum_{j}^{N} \hat{J}_{j}(1) - \hat{K}_{j}(1)
\end{equation}
then :
\begin{align}
	\hat{f}(1)\ket{\chi_i(1)} = \sum_{j}^{N} \varepsilon_{ij} \ket{\chi_{j}(1)} 
\end{align}


Because under unitary transformations two Slater determinant can differ only by a phase factor \cite[p.120]{Attila}, the Exchange and Coulomb operator are invariant under any transformation between two orthonormal sets $(\{\chi_j\}$, $\{\chi'_j\})$ .

Since :
\begin{equation}
	\bra{\chi_k}\hat{f}\ket{\chi_i} = \varepsilon_{ki}
\end{equation}

The matrix of Lagrange multipliers is the matrix representation of the Fock operator.
Because it is hermitian, it exists a set of orthonormal $\{\chi'_j\}$ that diagonalizes it.
By picking this set $\{\chi'_j\}$ as our base\footnote{We just need to know that they exists because their nature will be exposed by solving \eqref{eq:HartreeFockEquation}} we can rewrite equation \eqref{eq:HartreeNonCan} into:

\begin{align}
	\hat{f} \ket{\chi_i} = \varepsilon_{i} \ket{\chi_{i}} \label{eq:HartreeFockEquation}
\end{align}

Equation \eqref{eq:HartreeFockEquation} is called the \textit{Hartree-Fock canonical equation} and reduces our problem of finding the ground state of a given quantum system to a set of $K$ eigenvalue equations, with $K$ being the number of spin-orbitals selected to describe the system. 

$K$ must be at least equal to $N$, the number of electrons in the system, to respect the Pauli principle. The greater is $K$, the finer is our approximation, reaching what is called the ``\textit{Hartree-Fock limit}".

\subsubsection{Closed shell HF and the Roothaan equations} \label{sec:Roothan}

A set of spin orbitals is said to be \textit{restricted} if :


\[\chi_{i}(\mathbf{x}) = \left\{
  \begin{array}{lr}
    \psi_{j}(\mathbf{r}) \alpha(w)\\
    \psi_{j}(\mathbf{r}) \beta(w)
  \end{array}
\right.
\]

And the closed shell restricted ground state is :
\begin{align}
	\ket{\Psi_{0}} & = \ket{ \psi_{1} \overline{\psi}_{1} \cdots  
	\psi_{j} \overline{\psi}_{j}
	\cdots	
	\psi_{\frac{N}{2}} \overline{\psi}_{\frac{N}{2}}}
	\\
	\psi_{j} & = \psi_{j} \alpha
	\\
	\overline{\psi}_{j} & = \psi_{j} \beta
\end{align}

We can say that every spatial orbital $\psi(\mathbf{r})$ is occupied by both spin-up and spin-down electrons (there is no ``odd" electron alone in a spatial orbital).

Separating the integration of the spin function (which are orthonormal) and taking into account the correlation interaction between electrons with parallel spin (see section \ref{sec:slaterPropr}) \cite[p.132-133]{Attila} the Fock operator becomes :

\begin{equation}
\hat{f}(1) = \hat{h}(1) +\sum_{j=1}^{N/2}[2\hat J_j(1)-\hat K_j(1)]
\end{equation}

Where $J_j$ and $K_j$ are expressed as functions of the spatial orbitals $\{\psi_i\}$.

Thus the Hartree-Fock equation becomes:
\begin{equation}\label{eq:hfSpatial}
	\hat{f}(\mathbf{r_1}) \ket{\psi_i(\mathbf{r_1})} = \varepsilon_i \ket{\psi(\mathbf{r_1})}
\end{equation}

Instead of using a numerical approach to solve the integro-differential equation \eqref{eq:hfSpatial}, Roothaan showed how to convert the problem to a set of algebraic equations, solvable using matrix techniques.

We therefore introduce a set of K functions $\{ \phi_{\mu}(\mathbf{r}) \mid \mu = 0,\cdots,K\}$ (in general not orthogonal \footnote{Not orthogonal basis set are common. For example in a simple molecule one can take all the idrogenoid orbitals centered in each atom, then some overlap in the distance between adjacent nuclei is likely to occur.}) called a \textit{Basis set} \footnote{ The choice of the correct basis set has great impact on the solution of the electronic problem. A discussion on the nature and the \textit{art} of basis set selection is far from the goals of this work.} , that can be used to express our spin-orbitals $\{\psi_j\}$.

\begin{equation}\label{eq:coeffMatrix}
	\ket{\psi_i} = \sum_{\mu}^{K} C_{\mu i} \ket{\phi_{\mu}}
\end{equation}

Substituting inside \eqref{eq:hfSpatial} :
\begin{equation}
	\hat{f}(1) \sum_{\nu}^{K} C_{\nu i} \ket{\phi_{\nu}(1)}  = \varepsilon_i \sum_{\nu}^{K} C_{\nu i} \ket{\phi_{\nu}(1)}
\end{equation}

multiplying by $\bra{\phi_{\mu}}$ we get :

\begin{equation}
	 \sum_{\nu}^{K} C_{\nu i} \bra{\phi_{\mu}(1)}\hat{f}(1)\ket{\phi_{\nu}(1)}  
	 = \varepsilon_i \sum_{\nu}^{K} C_{\nu i} \bra{\phi_{\mu}(1)}\ket{\phi_{\nu}(1)}
\end{equation}

Taking under consideration every $\psi_i$ we can write the equation in matrix form:
\begin{equation}\label{eq:RoothaanEquation}
\boxed{
\mathbf{FC} = \mathbf{SC\varepsilon}
}
\end{equation}

Being :
\begin{itemize}
	\item $\mathbf{F}$ the Fock matrix expressed using our basis set
	\item $\mathbf{C}$ the matrix of the expansion coefficients
	\item $\mathbf{S}$ the overlap matrix between the orbitals of the basis set
	\item $\mathbf{\varepsilon}$ is the diagonal matrix of the orbitals energies	
\end{itemize}
All these matrices have dimension $K\times K$.

Equation \eqref{eq:RoothaanEquation} is called the \textit{Roothaan equation} and has a non-trivial solution only if the following  equation is satisfied \cite[p.309]{Atkins}:
\begin{equation}\label{eq:secular}
	\det \mid \mathbf{F - \varepsilon S} \mid =0
\end{equation}


\subsubsection{The Self Consistent Field Method} \label{sec:SCF}



We have finally obtained an expression of $\hat{V}_{ee}$ that keeps the electronic Hamiltonian \eqref{eq:H_elec} separated.

Each electron on the $i$-th spin-orbital is subject to the following external potential:
\begin{align}
	\hat{v}^{HF}(1) = \sum_{j}^{N} \hat{J}_{j}(1) - \hat{K}_{j}(1)
\end{align}

Rewriting the Hartree-Fock equation for the $i$-th electron we have:
\begin{align}
	\left[- \frac{\nabla_{i}^{2}}{2} + \sum_{\alpha}^{N_{N}} \frac{Z_{\alpha}}{r_{i\alpha}} + \sum_{j}^{N} \hat{J}_{j} - \hat{K}_{j}  \right] \ket{\chi_i}  & = \varepsilon_i \ket{\chi_i} \\
	\left[\hat{h} + \hat{v}^{HF}\right] \ket{\chi_i} & = \varepsilon_i \ket{\chi_i}
\end{align}

$\hat{h}$ is usually called the \textit{core Hamiltonian} because it represents the energy associated to the $i$-th electron under the potential generated uniquely by the nuclei.

$\hat{v}^{HF}$ is referred as an average potential, or \textit{mean field potential}, because it considers the effect of the average distribution of the $N-1$ electrons on the $i$-th electron. It also takes under consideration the effect of electron exchange thanks to the exchanges operator $\hat{K}_j$. Electronic correlation are fully neglected for the electrons of opposite spin, but are taken into account for electrons of parallel spin. \footnote{Electron exchange has not to be confused with electron correlation}

In the case of closed-shell Hartree-Fock we have restricted our problem only to spatial orbitals, and in a more handy matrix form expressed by equation \eqref{eq:RoothaanEquation}.

We still need to consider a very important property of the Fock operator.

As shown in equations \eqref{eq:coulombOperator} and \eqref{eq:exchangeOperator},
the Fock operator appears to be an implicit operator because it depends on the basis set we pick for the Slater determinant.
\begin{equation}
\hat{f} = \hat{f}[\{\psi_{j}\}]
\end{equation}

This is also true for the Fock matrix.

Because the spin-orbitals $\{\psi_{j}\}$ are expressed in terms of expansion coefficients, the Roothaan equation \eqref{eq:RoothaanEquation} can be rewritten as :
\begin{equation}
	\mathbf{F(C)~C} = \mathbf{SC\varepsilon}
\end{equation}


This forbids the possibility to solve the problem with a direct calculation. 
Because the equations appears to be non-linear, an iterative approach must be followed.

The iterative method outlined below will be explained in terms of Roothaan equation \eqref{eq:RoothaanEquation}, since it is  of greater interest from a computational point of view.

\begin{enumerate}
	\item We define the position of every nucleus that won't change until the electronic problem is solved \footnote{Remember that we are under Born-Oppenheimer approximation}
	\item We start by picking a basis set $\{\phi_{\mu}\}$ and guessing the matrix of expansion coefficients $\mathbf{C}$.
	\item We calculate all required molecular integrals in $\mathbf{S}$ and $\mathbf{F}$. Which implies the calculation of every coulomb and exchange integral, and the elements of the $\mathbf{H}^{core} = \{\bra{\phi_{\mu}}\hat{h}\ket{\phi_{\nu}}\}$ matrix.
	\item Equation \eqref{eq:secular} is solved to obtain matrix $\varepsilon$.
	\item A new coefficients matrix $\mathbf{C'}$ is obtained thanks to equation \eqref{eq:RoothaanEquation}.
	\item If the difference between $\mathbf{C'}$ and $\mathbf{C} $ is under our consistency threshold, we reached self-consistency and the computation is concluded. Otherwise the iteration is repeated from step 3 with $\mathbf{C'}$ as the new matrix of the expansion coefficients. Alternatively it is possible to check that the total energy has reached a minimum (even though usually only the lowest $\varepsilon_i$ is used in the comparison).
\end{enumerate}

\mynotes{ma una bella immaginetta???}

The iteration proceeds until the expansion coefficients are stable within multiple iterations. In reality the matrix $\mathbf{C'}$ is not used ``\textit{as is}"" to start the next iteration, it's usually combined with a subset of the previously obtained coefficients matrices. This is done to speed up the convergence towards self consistency.

When the iteration is concluded, an Hartree-Fock ground state functions is obtained. Coming back to the nuclear problem, the ground geometry of the molecule is obtained following the guidelines of the Born-Oppenheimer approximation.
A set of \textit{post Hartree-Fock} methods can be applied to obtain the excited electronic states, but this won't be covered in this work.

\subsubsection{The Fock matrix and the Density Matrix}
Although the iteration scheme seems rather simple, it is convenient to investigate the nature of the Fock matrix to better understand the computational complexity of SCF calculations.

Each Fock matrix element is calculated as :
\begin{align}
	F_{\mu \nu} & = \bra{\phi_{\mu}(1)}\hat{f}(1)\ket{\phi_{\nu}(1)}  \\
	& = \int \phi_{\mu}^*(\mathbf{r}_1) h(1) \phi_{\nu}(\mathbf{r}_1) \dd\mathbf{r}_1 + \\
	&  + 2 \sum_{j} \int \phi_{\mu}^*(1) \psi_{j}^*(2) \frac{1}{r_{12}} \psi_{j}(2) \phi_{\nu}(1) \dd\mathbf{r}_1  \dd\mathbf{r}_2  + \\
	&  - ~~ \sum_{j} \int \phi_{\mu}^*(1) \psi_{j}^*(2) \frac{1}{r_{12}} \phi_{\nu}(2) \psi_{j}(1)  \dd\mathbf{r}_1 \dd\mathbf{r}_2
\end{align}

by expanding $\psi_j$ and using the following definition \cite[p.295,296]{Atkins} \cite[p.141	]{Attila}
\begin{equation}
	(ab|cd) = \int \phi_a(1) \phi_b(1) \frac{1}{r_{12}}  \phi_c(2) \phi_d(2) \dd\mathbf{r}_1 \dd\mathbf{r}_2
\end{equation}

We obtain 

\begin{equation}\label{eq:FockElement}
	F_{\mu \nu} = H_{\mu \nu}^{core} +
		\sum_{j} \sum_{\lambda \sigma} C_{\lambda j} C_{\sigma j}^* \left[ 2(\mu \nu | \sigma \lambda) - (\mu \lambda | \sigma \nu)   \right]
\end{equation}

If we define \cite[p.139]{Attila}
\begin{equation}
	P_{\mu \nu} = 2 \sum_{j} C_{\lambda j} C_{\sigma j}^* 
\end{equation}

The final form of the Fock matrix element is :

\begin{equation} \label{eq:fockElement}
F_{\mu \nu} = H_{\mu \nu}^{core} +
		\sum_{j} \sum_{\lambda \sigma} P_{\mu \nu} \left[ (\mu \nu | \sigma \lambda) - \frac{1}{2} (\mu \lambda | \sigma \nu)   \right]
\end{equation}

The matrix $\mathbf{P}$ is called the \textit{density matrix}, because the total electronic density as a function of the position $\mathbf{r}$ can be expressed as follows:
\begin{equation}\label{eq:densityDef}
\rho (\mathbf{r}) = 2 \sum_{j}^{N/2} \psi_{j}^*(\mathbf{r}) \psi_{j}(\mathbf{r}) = \sum_{\mu \nu} P_{\mu \nu} \phi_{\mu}(\mathbf{r}) \phi_{\nu}(\mathbf{r})
\end{equation}

the element $P_{\mu \nu}$ are referred to as density matrix elements, and are interpreted as the total electron density in the overlap region of $\phi_{\mu}$ and $\phi_{\nu}$.
Because the number of integrals in \eqref{eq:fockElement} that has to be evaluated is of the order of $K^4$, where K is the size of our basis set $\{\phi_{\mu}\}$, even small basis sets for moderately-sized molecules can rapidly approach millions of two-electron integrals. 
Even tough some of of them would result null because of their mutual distance, or the calculation isn't needed because of molecular symmetries, their efficient calculation poses the greatest challenge in HF-SCF.


In equation \eqref{eq:FockOperator} we have re-expressed the Fock operator in terms of electron density, instead of the expansion coefficient matrix. 

When starting the HF-SCF method, instead of guessing the expansion coefficients $\mathbf{C}$, one could guess the electronic density and perform all the calculation accordingly.


This idea of shifting the problem towards the electron density is the main theme of the \textit{Density Functional Theory}.







\clearpage
\subsection{Density Functional Theory}

Density Functional Theory is among the most popular and versatile methods available in condensed-matter physics, computational physics, and computational chemistry.

Using this theory, the properties of a many-electron system can be determined by using functionals depending solely on the spatially dependent electron density.

In order to better understand DFT's foundations, it is convenient to introduce the atomic model developed by Thomas\cite{Thomas27} and Fermi\cite{Fermi27} in the late 20s. The model was later refined by Dirac including exchanges effects.

\subsubsection{The Thomas-Fermi Atomic Model}

The basic idea behind the TF model is to divide the space around the atom in a grid of boxes of equal length $l$, each box containing $\Delta N$ electrons which number can vary from box to box. 

The electrons will always behave like independent fermions and there is no interaction between boxes.
Finally, as we will later justify, the temperature of the system is 0 Kelvin \cite[p.47-51]{Parr}.

What we aim to do is to find an estimate of the energy of the system, and, hopefully, a more convenient method to calculate it.

Since the electrons, and the boxes, are completely independent, we can model each box as an infinite square well potential.

We know that each energy levels in the well is expressed as \cite[p.74]{Basdevant}:

\begin{equation}\label{eq:squareWellLevels}
	\varepsilon(n_{x},n_{y},n_{z}) = \frac{h^2}{8ml^{2}} (n_{x}^2+n_{y}^2+n_{z}^2)
\end{equation}
where $n_{x},n_{y},n_{z}$ are the quantum numbers associated with each level.
\footnote{Although their suffix one should not think of them as a ``real" spacial representation. What we are about to do is to model the space of possible energies as a vector space. This will bring many simplifications.}

By defining 
\begin{equation}
	R^2 = n_{x}^2+n_{y}^2+n_{z}^2
\end{equation}

we rewrite equations \eqref{eq:squareWellLevels} as:
\begin{equation}\label{eq:epsilon_R}
	\varepsilon(R) = \frac{h^2}{8ml^{2}} R^2.
\end{equation}

We want to have an estimate of the total number of energy levels below a certain threshold. 
If R is large enough we can approximate this number with the volume contained in an octet of a sphere:
\begin{equation}
	\Phi(R) = \frac{1}{8} \left( \frac{4}{3} \pi R^{3} \right)
\end{equation}

Using equation \eqref{eq:epsilon_R} we can express $\Phi$ as $\Phi(\varepsilon)$ 
\begin{equation}\label{eq:phi_varepsilon}
	\Phi(\varepsilon) = \frac{\pi}{6} \left( \frac{8 m l^2 \varepsilon}{h^2} \right)^{\frac{3}{2	}}
\end{equation}

We are interested in the variation of $\Phi(\varepsilon)$ around a fixed $\varepsilon$, this will give us the finest expression of the \textit{energy density} around $\varepsilon$. 
We expand \eqref{eq:phi_varepsilon} in series:

\begin{equation}
	\Phi(\varepsilon + \var{\varepsilon}) - \Phi(\varepsilon) = \left[ \frac{\pi}{4} \left( \frac{8 m l^2}{h^2} \right)^{\frac{3}{2}} \varepsilon^{\frac{1}{2}}  \right] ~ \var{\varepsilon} + \mathcal{O}((\var{\varepsilon})^2)
\end{equation}

The quantity between square braces represents the density of energy states around $\varepsilon$. 
It gives us an estimate of how many energy levels can be occupied by an electron at a certain energy.
\begin{equation}\label{eq:energyDensity}
	g(\varepsilon) =  \frac{\pi}{4} \left( \frac{8 m l^2}{h^2} \right)^{\frac{3}{2}} \varepsilon^{\frac{1}{2}}
\end{equation}

To obtain the number of energy levels contained in an energy interval, we integrate \eqref{eq:energyDensity}:

\begin{equation}
	\Delta \xi_{\varepsilon_1 \varepsilon_2} = \int_{\varepsilon_1}^{\varepsilon_2} g(\varepsilon) \dd{\varepsilon}
\end{equation}

To calculate to total energy of the box, we need to know how many levels are occupied by electrons. 
More precisely, we need the probability that a given energy level is occupied by an electron, namely the \textit{partition function} of the system. 

Since we are dealing with fermions, the Fermi-Dirac statistic must be used:

\begin{equation} \label{eq:fermiDiracPartitionFunction}
	f(\varepsilon) = \frac{1}{1 - e^{\beta (\varepsilon - \varepsilon_{f})}}
\end{equation}

Where $\varepsilon_f$ is the Fermi energy\footnote{ The fermi energy is the energy difference between the highest and lowest occupied single-particle states in a quantum system of non-interacting fermions at absolute zero temperature}. 

If the system is in its ground state we have that the number of occupied energy level around $\varepsilon$ is :
\begin{equation}
	2 f(\varepsilon) g(\varepsilon) \dd{\varepsilon}
\end{equation}
Because the electrons are non-interacting, every energy level can be occupied by two electrons with opposite spin, hence the factor $2$.

Finally to obtain the total energy of the box we multiply by $\varepsilon$ and integrate:
\begin{equation}\label{eq:deltaE}
	\Delta E = 2 \int f(\varepsilon) g(\varepsilon) \dd{\varepsilon}
\end{equation}

The calculation is greatly simplified if we consider the system to be at 0K. In this case $\beta = (k_B T)^{-1} \rightarrow + \infty$ and the Fermi-Dirac distribution \eqref{eq:fermiDiracPartitionFunction} becomes:

\begin{equation}
	f(\varepsilon) = \left\lbrace \begin{array}{ll}
		1 & \mbox{if $\varepsilon < \varepsilon_{f} $}  \\
		0 & \mbox{if $\varepsilon > \varepsilon_{f} $}
	\end{array}
	\right.
\end{equation}


Thus \eqref{eq:deltaE} becomes :
\begin{equation}
	\Delta E = 2 \int_{0}^{\varepsilon_F} g(\varepsilon) \dd{\varepsilon}
\end{equation}

Substituting \eqref{eq:energyDensity} in \eqref{eq:deltaE} and performing the integration we obtain the total energy of the box at 0K.
\begin{equation}
	\Delta E = 2 \int_{0}^{\varepsilon_F} g(\varepsilon) \dd{\varepsilon} = \frac{8\pi}{5} \left( \frac{2 m}{h^2} \right)^{\frac{3}{2}} l^3 \varepsilon_{f}^{\frac{5}{2}}
\end{equation}

We can express $\varepsilon_f$ as a function of the number of electrons in the cell :
\begin{equation}
	\Delta N = 2 \int f(\varepsilon) g(\varepsilon) \dd{\varepsilon} = \frac{8\pi}{3} \left( \frac{2 m}{h^2} \right)^{\frac{3}{2}} \varepsilon_{f}^{\frac{3}{2}}
\end{equation}

Then \cite{Parr}:
\begin{equation}
	\Delta E  = \frac{3}{5} \Delta N \varepsilon_{f}
\end{equation}
\begin{equation}
			  \Delta E = \frac{3}{10} \frac{h^2}{m} \left( \frac{3}{8\pi} \right)^{\frac{2}{3}} l^{3} \left( \frac{\Delta N}{l^{3}} \right)^{\frac{5}{3}}
\end{equation}

Since we neglected any possible interaction between electrons, the quantity $\Delta E$ represents the kinetic energy of the electrons inside each box.
If we lower the dimensions of each box and sum the contributes for all the boxes, we obtain:

\begin{equation}
	\frac{\Delta N}{l^{3}} \xrightarrow[l\rightarrow0]{} \rho(\mathbf{r})
\end{equation}
\begin{align}
	& T[\rho] = C_{F} \int \rho(\mathbf{r})^{\frac{5}{3}} \dd{\mathbf{r}}
	& C_{F} = \frac{3}{10}(3 \pi^{2})^{\frac{2}{3}} \label{eq:TFKinetic}
\end{align}

$\rho(\mathbf{r})$ is the spatial dependent electron density. $T[\rho]$ is the total kinetic energy of the system and it's a functional of the electron density $\rho(\mathbf{r})$.

We can now add the nuclear potential and the electron-electron Coulombic repulsion


\begin{equation}\label{eq:ThomasFermiEnergy}
	E_{tf}[\rho] = C_{F} \int \rho(\mathbf{r})^{\frac{5}{3}} \dd{\mathbf{r}} 
			- Z \int \frac{\rho(\mathbf{r})}{r} \dd{r} 
			+ \frac{1}{2} \iint \frac{\rho(\mathbf{r_{1}}) \rho(\mathbf{r_{2}}) }{r_{12}} \dd{\mathbf{r_1}} \dd{\mathbf{r_2}} .
\end{equation}

Now that we have an expression for the total energy that depends functionally on the electronic density\footnote{The Thomas-Fermi model neglects any exchanges and correlation effect.}, we can apply the variational principle to find the density that minimizes equation \eqref{eq:ThomasFermiEnergy}.

Our unique constraint is that the integral of the electronic density must be equal to the total number of electrons in the system
\begin{equation}\label{eq:TFDensityConstraint}
	N = \int \rho(\mathbf{r}) \dd{\mathbf{r}} .
\end{equation}
Then the equation for Lagrange multipliers takes the form:
\begin{equation}
	\var{} \lbrace  E_{tf}[\rho] - \mu_{tf} \left( \int \rho(\mathbf{r}) \dd{\mathbf{r}} - N \right) \rbrace
\end{equation}

By differentiating we have \cite[246-254]{Parr}
\begin{align}
	\mu_{tf} & = \frac{\var{E_{tf}}}{\var{\rho}}  \\
	\mu_{tf} & = \frac{5}{3} C_{F} \rho(\mathbf{r})^{\frac{2}{3}} - \frac{Z}{r} + \frac{1}{2} \int \frac{\rho(\mathbf{r})}{\mid \mathbf{r} - \mathbf{r_2} \mid} \dd{\mathbf{r_2}} \label{eq:TFEq}
\end{align}

We can define the \textit{electrostatic potential} due to the nucleus and the entire electron distribution as :
\begin{equation}
	\phi(\mathbf{r}) = \frac{Z}{r} - \int \frac{\rho(\mathbf{r_2})}{\mid \mathbf{r} - \mathbf{r_2} \mid} \dd{\mathbf{r_2}}
\end{equation}
and write :
\begin{equation}
\mu_{tf} = \frac{5}{3} C_{F} \rho(\mathbf{r})^{\frac{2}{3}} - \phi(\mathbf{r})
\end{equation}


Equation \eqref{eq:TFEq} can be solved in conjunction with \eqref{eq:TFDensityConstraint}, and the resulting $\rho(\mathbf{r})$ then inserted in \eqref{eq:ThomasFermiEnergy} to give the total atomic energy.

As in the case of the pure Hartree method we are simplifying the model by neglecting any exchange and correlation effect. 

The correction to this approximation was introduced by Dirac in the Thomas-Fermi-Dirac atomic model.



\subsubsection{Thomas-Fermi-Dirac Atomic Model}

The contribute of P.A.M. Dirac was to model the exchanges term\footnote{Correlation effects are still neglected.} for a closed shell system as follows:
\begin{equation}\label{eq:diracExc}
	K[\rho] = \frac{1}{4} \iint \frac{\mid \rho(\mathbf{r_1},\mathbf{r_2})\mid^2 }{r_{12}} \dd{\mathbf{r_1}} \dd{\mathbf{r_2}},
\end{equation}

with

\begin{equation} \label{eq:excDensity}
	\rho(\mathbf{r_1},\mathbf{r_2}) 
	= 2 \sum_i 
	\psi_{i}^{*}
	(\mathbf{r_1}) 
	\psi_{i}
	(\mathbf{r_2}).
\end{equation}

The square module in \eqref{eq:diracExc} is needed because \eqref{eq:excDensity} can be imaginary.

Note that expression \eqref{eq:excDensity} depends on two spatial coordinates.
This non-local nature of \eqref{eq:diracExc} poses a great challenge in the solution of equation \eqref{eq:TFEq}.

By using plane waves as spatial orbitals \footnote{Remeber that the single particle is contained in an infinite square well potential.} 
\begin{align}
	\psi(k_x,k_y,k_z) = \frac{1}{l^{\frac{3}{2}}} e^{i(k_x x + k_y y + k_z z)}
\end{align}
\begin{align}
		k_x & =\frac{2\pi}{l} n_x & k_y & =\frac{2\pi}{l} n_y & k_z & =\frac{2\pi}{l} n_z
\end{align}



and introducing the following periodic boundaries conditions:

\begin{equation}
\left\lbrace \begin{array}{ll}
		\psi(0) = 0 \\
		\psi(l) = 0 
	\end{array}
	\right.
\end{equation}
\begin{equation}
	\psi(x+l) = \psi(x)
\end{equation}


We can re-express \eqref{eq:diracExc} as \cite[p.105-109]{Parr} :

\begin{align}
	& K[\rho] = C_{x} \int \rho^{\frac{4}{3}}(\mathbf{r}) \dd{\mathbf{r}} & C_{X} = \frac{3}{4} \left(\frac{3}{\pi} \right)^{\frac{1}{3}} \label{eq:DiracK}
\end{align}


This process is an example of \textit{local density approximation} or LDA, consisting in the reduction of the dependency of $K$ to a single spatial coordinate.

We can write the energy functional as :

\begin{equation}
	E_{tfd}[\rho] = C_{F} \int \rho(\mathbf{r})^{\frac{5}{3}} \dd{\mathbf{r}} 
			- Z \int \frac{\rho(\mathbf{r})}{r} \dd{r} 
			+ \frac{1}{2} \iint \frac{\rho(\mathbf{r_{1}}) \rho(\mathbf{r_{2}}) }{r_{12}} \dd{\mathbf{r_1}} \dd{\mathbf{r_2}}
			- C_{x} \int \rho^{\frac{4}{3}}(\mathbf{r}) \dd{\mathbf{r}}
\end{equation}

Then equation \eqref{eq:TFEq} becomes:
\begin{equation}
\mu_{tf}  = \frac{5}{3} C_{F} \rho(\mathbf{r})^{\frac{2}{3}} - \frac{Z}{r} + \frac{1}{2} \int \frac{\rho(\mathbf{r})}{\mid \mathbf{r} - \mathbf{r_2} \mid} \dd{\mathbf{r_2}} - \frac{4}{3	} C_{X} \rho(\mathbf{r})^{\frac{1}{3}}.
\end{equation}

Because the models completely neglects correlation effects, the results obtained are not satisfying, but the TFD model is the springboard for DFT development.



\subsubsection{Hohenberg and Kohn Theorems}

The TFD model lead the way in a different kind of approach to the electronic problem. 
The importance of the electronic density became central and extending the TFD model to molecular systems was the next step.

If it was certain that an external potential $v(\mathbf{r})$\footnote{Which nature is to be considered as general as possible.} acting on electrons describes an unique electronic density $\dens$ \cite[p.318]{Atkins}, there was no formal proof that the contrary was true.

Can an electronic density function $\rho(\mathbf{r})$ be associated with an unique external potential $v(\mathbf{r})$?

The nature of the question is not just formal. 
Every physical and chemical property of the molecule is determined by it's geometry, which is shaped by the external potential.
If the external potential is univocally associated with a single electronic density, by finding $\rho(\mathbf{r})$ one wold determine all the properties of the system, including the total energy expressed as a functional $E =E[\rho(\mathbf{r})]$.

The question remained open until 1964, when Hohenberg and Kohn published their milestone paper \cite{HK} containing two fundamental theorems.

The fist theorem states that an external potential ``\textit{$v(\mathbf{r})$ is an unique functional of $\rho(\mathbf{r})$ }".
The proof outlined in \cite{HK} proceeds by \textit{reductio ab absurdum} and, despite its simplicity, its rather powerful.

The second theorem states that the variational principle can be applied to system described by the electronic density.
It can be formulated as : 

\textit{Given a trial density function $\rho'(\mathbf{r})$ for which: }

\begin{itemize}
	\item $\mid \rho'(\mathbf{r}) \mid^2$ 
	\item $N = \int \rho'(\mathbf{r}) \dd{\mathbf{r}}$
\end{itemize}

\textit{Then}
\begin{equation}
	E_0 \leq E[\rho'(\mathbf{r})]
\end{equation}

\textit{where $E_0$ is the ground state energy of the system.}

We can now express more explicitly the $E[\dens]$, in analogy with the TFD model.

\begin{align}
	E[\rho(\mathbf{r})] &= E_{HK}[\dens] + \int \rho(\mathbf{r}) v(\mathbf{r}) \dd{\mathbf{r}} \\
	E_{HK}[\dens] &= T[\rho(\mathbf{r})] + V_{ee}[\rho(\mathbf{r})] \\
	v(\mathbf{r}) &= \sum_{\alpha} \frac{Z_{\alpha}}{\mid \erre - \erre_{\alpha} \mid}
\end{align}

$E_{HK}$ is a universal functional of $\dens$ and the contribute $V_{ee}$ contains both the Coulombian and the excahnges-correlation terms.

By applying the variational principle with the usual constraint \eqref{eq:TFDensityConstraint}, the fundamental equation of DFT is obtained:

\begin{equation}\label{eq:FoundamentalDFT}
\boxed{
	\mu_{HK} = v(\erre) + \frac{\var{E_{HK}[\dens]}}{\var{\dens}}
}
\end{equation}


\subsubsection{Kohn and Sham Equations}

Kohn and Sham \cite{KS} developed a practical method to solve equation \eqref{eq:FoundamentalDFT}.

Given a \textit{real} system one could associate a \textit{reference} system in which all electrons are not correlated, but with the same electronic density $\dens$.

In such a system, all electrons are subject to an external potential $v_{ref}(\erre)$, depending only on the spatial coordinate $\erre$.\footnotemark

\footnotetext{As in the HF model, the external potential can be considered an average potential acting on each electron independently.}

In analogy with the HF method,\footnote{See section \ref{sec:HF}.} the Hamiltonian of the system is completely separable

\begin{align}
\hat{H}_{ref} &= \sum_{i} \hat{h}_i^{KS} \\
\hat{h}_i^{KS} &= - \frac{\nabla^{2}_{i}}{2} + v(\erre)_{ref} .
\end{align}

The solutions to the Schr\"odinger equations for spatial orbitals are
\begin{equation} \label{eq:RawKSeq}
	\hat{h}_i^{KS} \ket{\psi_{i}^{KS}} = \varepsilon_{i}^{KS} \ket{\psi_{i}^{KS}} ,
\end{equation}
and the properly anti-symmetric ket state of the system is \footnote{We consider only closed shell systems.}
\begin{equation}
	\ket{\Psi^{KS}} = \mid \chi_{1}^{KS} \cdots \chi_{2N}^{KS} \mid .
\end{equation}


To ``\textit{link}" the reference system and the real system together, one could write the energy functional of the real system and add and subtract the contributions of the reference system.
\begin{align}
	E[\dens] &= T[\dens] + V_{ee}[\dens] + \int \dens v(\erre) \dd{\erre} \\
			 &= T_{ref}[\dens] + J_{ref}[\dens] + \int \dens v(\erre) \dd{\erre} \nonumber \\ 
			 &+ \lbrace
			 	T[\dens] + V_{ee}[\dens] - 
			 	\left( 
			 		T_{ref}[\dens] + J_{ref}[\dens] 
			 	\right)
			 \rbrace \\
			 &= T_{ref}[\dens] + J[\dens] + \int \dens v(\erre) \dd{\erre} \nonumber \\ 
			 &+ \lbrace
			 	T[\dens] + V_{ee}[\dens] - 
			 	\left( 
			 		T_{ref}[\dens] + J[\dens] 
			 	\right)
			 \rbrace, \label{eq:KSEnergy}
\end{align}

where in the last step we have dropped the $ref$ suffix from $J_{ref}$ because $J$ is a universal functional of $\dens$ and has the same form in both the systems \footnote{Remember that the the reference system has the same $\dens$.}.

We define the \textit{exchanges energy functional} as 
\begin{equation}\label{eq:KSExchangeEnergy}
E_{xc}[\dens] = T[\dens] + V_{ee}[\dens] - 
			 	\left( 
			 		T_{ref}[\dens] + J[\dens] 
			 	\right)
.
\end{equation}

Substituting \eqref{eq:KSExchangeEnergy} in \eqref{eq:KSEnergy}, equation \eqref{eq:FoundamentalDFT} becomes 
\begin{equation}\label{eq:KSChemPot} 
	\mu_{KS} = \frac{\var{T_{ref}[\dens]}}{\var{\dens}}  + v_{eff}(\erre),
\end{equation}
with 
\begin{align}
 v_{eff}(\erre) &= v(\erre)  + v_{h}(\erre)
 + v_{xc}(\erre) \\
 v_{h}(\erre) &= \int \frac{\rho(\mathbf{r'})}{\mid \mathbf{r} - \mathbf{r'} \mid}  \dd{\mathbf{r'}}\\
 v_{xc}(\erre) &= \frac{\var{E_{xc}[\dens]}}{\var{\dens}}.
\end{align}
$v_{h}$ is called the \textit{Hartree potential} and the associated energy 
\begin{equation}\label{eq:HartreeEnergy}
E_{h}[\dens] = \frac{1}{2} \iint \frac{\rho(\mathbf{r_{1}}) \rho(\mathbf{r_{2}}) }{r_{12}} \dd{\mathbf{r_1}} \dd{\mathbf{r_2}} = J[\dens]
\end{equation}
 is called the \textit{Hartree energy}.


As one can see, $v_{eff}$, the \textit{effective potential}, is not a functional of $\dens$, but a real function of $\erre$
\footnote{Note that in the definition of $v_{eff}$ we have already performed $\frac{\var{J}}{\var{\rho}}$.}.

This observation is fundamental because equation \eqref{eq:KSChemPot} \footnote{Which is valid for the real system.} is the same equation that we would have obtained with a system composed by electrons subjected only to an external potential $v_{eff}(\erre)$.

Then the electronic density satisfying equation \eqref{eq:KSChemPot} is the same obtained by solving equations \eqref{eq:RawKSeq} with $v_{eff}$ instead of $v_{ref}$.

\begin{equation} \label{eq:KSeq}
\boxed{
	\lbrace h_{1} + v_{h}(\erre)+ v_{xc}(\erre) \rbrace 
		\psi_{j}^{KS}(\erre) 
	= \varepsilon_{j}^{KS} \psi_{j}^{KS}(\erre)
},
\end{equation}
where 
\begin{align}
	h_{1} &= - \frac{\nabla_{1}^2}{2} + v(r)\\
	v(r) &= \sum_{\alpha} \frac{Z_{\alpha}}{r} .
\end{align}

Equations \eqref{eq:KSeq} are called the \textit{Kohn-Sham equations}, once solved it's possible to calculate the electronic density using definition \eqref{eq:densityDef}.

However, in order to proceed, it is necessary to choose a proper form of $E_{xc}[\dens]$ (consequently of $v_{xc}(\erre)$), which is far from being an easy task. The search for more accurate functionals is an active area of current research efforts.

The main problem arises from the approximate nature of $E_{xc}$, which is usually split in an exchanges part $E_{x}$ and a correlation part $E_c$.

In the most primitive local density approximation (LDA), we use Dirac expression for the exchange energy, whereby the exchanges contribution is given by $E_{x}[\dens] = -\frac{1}{2} K[\dens]$ and $K[\dens]$ is the one in \eqref{eq:DiracK} with an adjustable parameter $\alpha$:
\begin{align}
	&E_{xc} = A \int \dens^{\frac{4}{3}} \dd{\erre} &A= -\frac{3}{8}\alpha \left( \frac{3}{\pi} \right)^{\frac{1}{3}}
\end{align}

As show in \eqref{eq:DiracK} the exchanges potential has the form:
\begin{align}
	v_{xc}(\erre) &=	\frac{\var{E_{xc}[\dens]}}{\var{\dens}}\\
	&= \frac{4}{3} A \dens^{\frac{1}{3}} .
\end{align}



To proceed with the solution of equations \eqref{eq:KSeq} it is convenient to rewrite it in matrix form. 

Following the same steps explained in section \ref{sec:Roothan} we obtain:

\begin{equation}\label{eq:DFTMatrix}
	\mathbf{H}^{KS} \mathbf{C} = \mathbf{S C} \varepsilon,
\end{equation}

where 
\begin{equation}\label{eq:DFTH}
	\mathbf{H}^{KS}_{ij} = \int \psi^{*}_{i}(\erre) \lbrace h_{1} + v_{h}(\erre)+ v_{xc}(\erre) \rbrace \psi_{j}(\erre) \dd{\erre},
\end{equation}
$\mathbf{C}$ is the matrix of expansion coefficients \eqref{eq:coeffMatrix} and $\mathbf{S}$ is the overlap matrix.

We can see that from a computational standpoint there is a deep difference between the $\mathbf{H}^{KS}$ matrix and the Fock matrix $\mathbf{F}$: evaluation of $\mathbf{H}^{KS}_{ij}$ is much easier than $\mathbf{F}_{ij}$ because there is no need to evaluate the onerous exchange integrals $\sum_{\nu \lambda}^{K}(\mu \lambda | \sigma \nu)$ in \eqref{eq:FockElement}. 
All the exchanges-correlation contribution is reduced to a multiplicative potential $v_{xc}(\erre)$.

In fact the complexity associated to DFT calculation is lower than a standard HF calculation, but it depends strongly on the choice of basis set. Please refer to section \ref{sec:QE} for a more in-depth analysis.

Before proceeding further it is convenient to reformulate the total energy functional $E[\dens]$.

By defining 
\begin{equation}
	V[\dens] = \int \dens v(\erre) \dd{\erre}
\end{equation}
and substituting \eqref{eq:HartreeEnergy} in equation \eqref{eq:KSEnergy}, the energy functional becomes:
\begin{equation}\label{eq:EnergyFuntional}
	E[\dens] = T_{ref}[\dens] + E_{h}[\dens] + V[\dens] + E_{xc}[\dens].
\end{equation}

Once $\dens$ is obtained by solving \eqref{eq:KSeq}, one still have to provide an expression of $T_{ref}[\dens]$. 
Although the Thomas-Fermi model can provide an approximation \eqref{eq:TFKinetic}, we can use equations \eqref{eq:KSeq} to obtain a more convenient expression.

For a diagonal set of spatial orbitals $\{\psi^{KS}_{j}\}$\footnote{Guaranteed to exist after the diagonalization of \eqref{eq:DFTH}}, the expectation value for the total Energy has the form :
\begin{align}
	& \langle E \rangle = \sum_{j} \bra{\psi^{KS}_{j}}\mathbf{H}\ket{\psi^{KS}_{j}} \\
	&= T_{ref} + \sum_{j} \bra{\psi^{KS}_{j}}v\ket{\psi^{KS}_{j}} + \sum_{j} \bra{\psi^{KS}_{j}}v_{h}\ket{\psi^{KS}_{j}} + \sum_{j} \bra{\psi^{KS}_{j}}v_{xc}\ket{\psi^{KS}_{j}} = 
	\sum_{j} \varepsilon^{KS}_{j} .
\end{align}

Then 
\begin{equation}\label{eq:Tref}
T_{ref} = - \sum_{j} \bra{\psi^{KS}_{j}}v\ket{\psi^{KS}_{j}} - \sum_{j} \bra{\psi^{KS}_{j}}v_{h}\ket{\psi^{KS}_{j}} - \sum_{j} \bra{\psi^{KS}_{j}}v_{xc}\ket{\psi^{KS}_{j}} + \sum_{j} \varepsilon^{KS}_{j} .
\end{equation}

Re-expressing \eqref{eq:EnergyFuntional} in the $\{\psi^{KS}_{j}\}$ base and substituting \eqref{eq:Tref} in \eqref{eq:EnergyFuntional}, we finally obtain :
\begin{align}\label{eq:DFTEnergy}
	E[\dens] = \sum_{j}\varepsilon^{KS}_{j} - E_{h}[\dens] - \int \dens v(\erre) \dd{\erre}+ E_{xc}[\dens].
\end{align}
\mynotes{Per Davide: l'ho controllata, se si definishe $E_{h}$ con il fattore $\frac{1}{2}$ e' giusta cosi'}
Equation \eqref{eq:DFTEnergy} is the one used by the vast majority of \textit{ab initio} calculations softwares because it's composed by terms that are already computed at the end of every SCF iteration.





\subsubsection{Self Consistent DFT}
Because $v_{h}$ and $v_{xc}$ in \eqref{eq:KSeq} implicitly depend on the orbitals set trough $\dens$ (see eq. \eqref{eq:densityDef}), an interative SCF approach must be used. 

We briefly outline the general procedure used in DFT-SCF calculations.

To begin the calculation we need to pick the correct approximation of $E_{xc}[\dens]$ from which we calculate $v_{xc}(\erre)$. $E_{xc}[\dens]$ will remain always constant trough the whole calculation.

In contrast to the Hartree-Fock SCF method, instead of guessing the orbitals (or their expansion coefficients), we guess the electronic density $\dens$. 
Usually, as a first rough approximation, the electronic densities of each atomic specie are used.\footnote{Note that any initial guess is equally valid, but a more reasonable one will make the iteration converge faster.}

The SCF iteration can now begin. 
We will call the initial density $\rho^{in}(\erre)$ and  the density obtained at the end of each iteration step will be $\rho^{out}(\erre)$.


\begin{enumerate}
	\item $\rho^{in}(\erre)$ is used to evaluate $v_{eff}(\erre)$;
	\item the eigenvalue problem \eqref{eq:DFTMatrix} is solved and the set of $\frac{N_{e}}{2}$ orbitals $\{ \psi^{KS}_i \}$  (the ones with the lowest eigenvalues $\varepsilon^{KS}_i$) is obtained; \label{en:SCFDiag}
	\item $\{ \psi^{KS}_i \}$ is used to calculate the new electronic density $\rho^{out}(\erre)$ trough definition \eqref{eq:densityDef};
	\item self consistency is tested: 
	
	usually the value $\mid\mid \rho^{out} - \rho^{in} \mid\mid = \int \mid \rho^{out}(\erre) - \rho^{in}(\erre)\mid \dd{\erre} $ is evaluated in conjunction with the difference between energies $E^{in}[\dens] - E^{out}[\dens]$ obtained from \eqref{eq:DFTEnergy}, but a wide range of methods can be used depending on the implementation. 
	\item if self-consistency is achieved the computation is completed and the final electronic density is the solution of equation \eqref{eq:FoundamentalDFT}. Otherwise $\rho^{out}(\erre)$ is used to determine the next $\rho^{in}(\erre)$.To guarantee a faster and more stable SCF convergence, $\rho^{out}(\erre)$ is mixed with the electronic densities obtained in previous iterations (if any).
\end{enumerate}


\begin{figure}[h]
	\includegraphics[width=\linewidth]{SCF-DFT_schema.pdf}
	\caption{DFT SCF procedure layout}
\end{figure}



As one can imagine, the main computational complexity is contained in step \ref{en:SCFDiag}.
The choice of the basis set has great impact on the implementation of this procedure, both for the evaluation of $H \ket{\psi}$ and the diagonalization strategy. 

As a final consideration of this section we must say that the Kohn-Sham eigenvalues $\varepsilon^{KS}_j$ don't posses a physical meaning \cite[p.144]{Martin}, they cannot be associated with \textit{real} energies \footnote{With the exception of the highest $\varepsilon_{i}$ \cite[p.144]{Martin}}. 
The same can be said about the spatial orbitals  $\{\psi^{ks}_j\}$, which are not \textit{real} electronic orbitals. 
Nevertheless they constitute a set of useful tools to build other meaningful physical quantities, first and foremost the electronic density $\dens$.

\subsection{Lattice Structures}
It is convenient to introduce the concept of \textit{Bravais lattice} and \textit{reciprocal lattice} for their central role in solid state physic. 


\subsubsection{Bravais Lattice}
The \textit{Bravais lattice} is a set of points in real space that can be identified by the following relation :
\begin{equation}\label{eq:BravaisLattice}
	\mathbf{R}_n = \sum_{i}^{D} n_{i} \mathbf{a}_{i},
\end{equation}
where $D$ is the number of dimensions, $\{\mathbf{a}_{i}\}$ is as a set of $D$ linear independent vectors called the \textit{primitive vectors} and $n_{i}$ is always an integer number.
The definition appears clear looking at figure \ref{fig:BravaisLattice}

\begin{figure}[h]
\begin{center}
	\includegraphics[width=0.4\linewidth]{Bravais_2D.png}
	\caption{Bi-dimensional Bravais lattice, with two of the possible primitive vectors.}
	\label{fig:BravaisLattice}
\end{center}
\end{figure}

\subsubsection{Primitive Cell}
The \textit{primitive cell} is the single smallest portion of lattice that can generate the whole lattice by applying translation and symmetry operations.

\subsubsection{Wigner-Seitz Cell}

The \textit{Wigner-Seitz} cell around a lattice point $\mathbf{R}$ is defined as the locus of points in space that are closer to that lattice point than to any of the other lattice points (see figure \ref{fig:WSCell}).

\begin{figure}[h]
\begin{center}
	\includegraphics[width=0.4\linewidth]{Wigner.pdf}
	\caption{Bi-dimensional Wigner-Seitz cell}
	\label{fig:WSCell}
\end{center}
\end{figure}

\subsubsection{The Reciprocal Lattice}
Considering a set of $\mathbf{R}$ points constituting a Bravais lattice and a plane wave $\psi(\erre) = e^{i\mathbf{k}\cdot\mathbf{r}}$, for a generic $\mathbf{k} \in \mathbb{R}$ the plane wave $\psi(\erre)$ will be periodic on the lattice only for certain values of $\mathbf{k}$.


We call the ``\textit{reciprocal lattice}" the set $\{\mathbf{K}\}$ of points $\mathbf{k}$ for which the associated plane wave $\psi(\erre)$ is periodic in the Bravais lattice.

One can show \cite{Martin} that the reciprocal lattice is a Bravais lattice and that the following relationship (in matrix form) between the reciprocal primitive vectors $\{\mathbf{b}_i\}$ and the original primitive vectors $\{\mathbf{a}_i\}$ held:
\begin{equation}
\left[\mathbf{b_{1}b_{2}b_{3}}\right]^{T} = (2\pi^3) \left[\mathbf{a_{1}a_{2}a_{3}}\right]^{-1}
.
\end{equation}

A generic point in the reciprocal lattice is identified by the $\mathbf{G_m}$ vector 
\begin{equation}
	\mathbf{G_m} = m_1\mathbf{b_1} + m_2\mathbf{b_2} + m_3\mathbf{b_3}.
\end{equation}

Given the periodic condition, every plane wave must satisfy 
\begin{equation}
	e^{i \mathbf{G}\cdot (\mathbf{r} + \mathbf{R})} =  	e^{i \mathbf{G}\cdot \mathbf{r}},
\end{equation}
hence 
\begin{equation}
	e^{i \mathbf{G}\cdot \mathbf{R}} =  	1,
\end{equation}
equivalently 
\begin{align}
	&\mathbf{G}\cdot \mathbf{R} = 2\pi N, &N\in\mathbb{Z},
\end{align}
for every $\mathbf{R}$ in the Bravais lattice.

Usually the original Bravais lattice $\{ \mathbf{R} \}$ is called the ``\textit{direct}" lattice.

Finally, the Wigner-Seitz cell of the reciprocal lattice is called the ``\textit{first Brillouin zone}".

As explained in \cite[p.121]{Manini} the reciprocal lattice is the (discrete) Fourier transform of the direct lattice.

We will see that the duality between the direct lattice and the reciprocal lattice is one of the greatest computational strength of theories based on plane waves basis sets.

\subsection{Electrons in Lattice}

Solid state physics studies the properties of matter when arranged in crystal lattice form, i.e. each lattice point (or node) accommodate an atom or a molecular structure. 
Elementary cells composed by a group of nodes are then replicated trough translation and symmetry operations to generate the whole lattice.
\subsubsection{The Bloch Theorem}
Since, as we have seen before, the potential the electrons are subjected to is mainly defined by ions' positions \footnote{Remember that the Born-Oppenheimer approximations stands firmly.} we must expect a periodic $v_{eff}$ potential.

Namely, we have that in equation \eqref{eq:KSeq} 
\begin{equation}
	v_{eff}(\erre) = 	v_{eff}(\erre + \mf{R}).
\end{equation}
But what are the implications of this condition on the solutions of \eqref{eq:KSeq}?

The well celebrated \textit{Bloch's theorem} \cite[p.136]{Manini} answers the question:

\begin{center}
\begin{framed}

\textit{All Schr\"odinger eigenstates in a periodic potential $v_{eff}(\erre) = 	v_{eff}(\erre + \mf{R})$ can be chosen in the factorized form:}
\begin{equation}\label{eq:Bloch}
		\psi_{j}(\erre) = e^{i \mf{k} \cdot \erre } ~ u_{\mf{k}j}(\erre), 
\end{equation}
\textit{where the function $u_{\mf{k}j}(\erre)$ has the same periodicity of the lattice $u_{\mf{k}j}(\erre) = u_{\mf{k}j}(\erre + \mf{R}) $, and $\mf{k}$ is a suitable wave vector (depending on $\psi_j$, but otherwise subject to no restriction).}
\end{framed}
\end{center}

Which implies two equivalent considerations.
\begin{itemize}
	\item In a periodic context all electronic eigenfunctions have non-trivial spatial  dependence only within one primitive unit cell: in any other cell the wave function is equal to that in the original cell, apart from a constant phase factor $e^{i \mf{k} \cdot \mf{R}}$ (which leaves the probability distribution unaffected).
	\item In a periodic potential, the Schr\"odinger eigenstates are essentially plane-wave-like states, except for a periodic amplitude modulation.
\end{itemize}

If we substitute \eqref{eq:Bloch} in the Kone-Sham equations \eqref{eq:KSeq} we obtain \cite[p.137]{Manini}
\begin{framed}
\begin{equation}\label{eq:BlochEq}
	\left\lbrace -\frac{1}{2}  \left( \nabla + i\mf{k} \right)^2 + v_{eff}(\erre) \right\rbrace u_{\mf{k}j}(\erre) = \varepsilon_{\mf{k}j}  u_{\mf{k}j}(\erre),
\end{equation}
\end{framed}
which is the basic equation for stationary states of an electron characterized by a given $\mf{k}$.\footnote{Note that this result is the same for every ``\textit{mean field}" approximation (i.e. HF equations)}

The consequence of the Bloch's theorem is that, thanks to the periodicity of $u_{\mf{k}j}(\erre)$, equation \eqref{eq:BlochEq} must be solved within a single cell of the direct lattice\footnote{With applied boundaries conditions}.
This is to be compared with the solution of equations \eqref{eq:KSeq} that must be solved in the \textit{whole} crystal lattice.

Once $u_{\mf{k}j}(\erre)$ is found, the true electronic wave function $\psi_j(\erre)$ is extended to the whole lattice trough \eqref{eq:Bloch}.

Apart for the $\mf{k}$ shift of wave vector, equation \eqref{eq:BlochEq} is equivalent to a stationary Schr\"oendiger equation. Then, for a fixed $\mf{k}$, its solutions must be qualitatively similar to those of standard Schr\"oendiger equations in a finite volume, namely : a ladder of discrete eigenenergies $\varepsilon_{\mf{k}j}$ associated with eigenfunctions $u_{\mf{k}j}(\erre)$ with larger and larger number of nodes for increasing energies, labeled precisely by the index $j$.\footnote{With a noticeable parallelism to atomic orbitals.}

It is absolutely fundamental to note that $\mf{k}$ can vary freely within the first Brillouin zone; 
then for a fixed $j$ the eigenenergies $\varepsilon_{\mf{k}j}$ depends on $\mf{k}$ as \textbf{continuous} functions!

These function are called ``\textit{energy bands}" or simply ``\textit{bands}", because they varies with $\mf{k}$ within a certain energy interval. 
In general, in solids, bands of contiguous $j$ can either overlap or not; 
thus the main consequence of the Bloch's theorem is that the allowed energies forms a spectrum of bands, separated by ranges of forbidden energies (\textit{band gaps}).

\subsubsection{Plane Waves}
In a crystals, electrons are characterized by an energy spectrum somewhat intermediate between that of a free particle (plane waves with all positive energies) and that of an atom (isolated eigenvalues separated by gaps).

One possible approach to model this kind of behavior is to use plane waves as basis set for the expansion of $\psi_{j}$.
\begin{equation}\label{eq:PWExpansion}
	\ket{\psi_j}  = \sum_{\mf{k}'} c_{j\mf{k}'} \ket{\mf{k}'},
\end{equation}
where $\{\ket{\mf{k}'}\}$ is a set of ortho-normalized plane wave states $\mathbf{k'}(\erre) = \sqrt{V}^{-1} e^{i\mf{k'}\cdot\erre}$ and $c_{j\mf{k}'}$ are the Fourier expansion coefficients.\footnote{Note that in general the sum extends over all possible $\mf{k}'$, leading to an integration:
\begin{equation}
	\psi_j(\erre) = \int \varphi_j(\mf{k}) e^{i\mf{k} \cdot \erre} \dd{\mf{k}}
\end{equation}
}

Replacing \eqref{eq:PWExpansion} in \eqref{eq:KSeq} and multiplying on the left for $\bra{\mf{k}}$ we obtain \cite{Martin} 
\begin{align}
	\bra{\mf{k}} -\frac{1}{2}\nabla^2 + v_{eff} \ket{\psi_j} &= \varepsilon_j \bra{\mf{k}}\ket{\psi_j}  \\
	\sum_{\mf{k'}} \left( \frac{1}{2}\mid\mf{k}\mid^2  \delta_{\mf{k},\mf{k'}} + \bra{\mf{k}}v_{eff}\ket{\mf{k'}} \right) c_{\mathbf{jk'}} &= \varepsilon_j \sum_{\mf{k'}} \delta_{\mf{k},\mf{k'}} c_{\mathbf{jk'}} \label{eq:SCHFourier}
\end{align}

where we have used $\bra{\mf{k}}\ket{\mf{k'}} = \delta_{\mf{k},\mf{k'}}$ and the fact that $\ket{\mf{k}}$ are eigenstates of the momentum operator.

This the form equation \eqref{eq:DFTMatrix} takes when plane waves are used as basis set, where $\mathbf{S}$ is diagonal and $\mathbf{C}$ is composed by Fourier expansion coefficients.

Expliciting the matrix element $\bra{\mf{k}} v_{eff} \ket{\mf{k'}}$ we have \cite{Martin} 
\begin{equation}
	\bra{\mf{k}} v_{eff} \ket{\mf{k'}} = \mathcal{N} \int e^{i\left(\mf{k} - \mf{k'} \right)} v_{eff}(\erre) \dd{\erre} = \tilde{v}_{eff}(\mf{k} - \mf{k'}),
\end{equation}

that are the Fourier components of the potential $v_{eff}$, where $\mathcal{N}$ is the appropriate normalizing constant.

So far we said nothing about the nature of our system, in fact equation \eqref{eq:SCHFourier} is the equivalent formulation of \eqref{eq:KSeq} in Fourier space, where in general $\mf{k}$ are continuous quantities.
This means that by using plane waves we can perform a set of calculations in the reciprocal space, and then transpose the results in the direct space trough inverse Fourier transformations.

If we introduce the condition of a periodic potential, as a consequence of the Bloch's theorem, we have that $\tilde{v}_{eff}(\mf{k} - \mf{k'})$ is not-null only when $\mf{k} - \mf{k'} = \mf{G}$, where $\mf{G}$ is a vector of the reciprocal lattice \cite{Martin}.

This means that in the continuous-indexed energy matrix of eq \eqref{eq:SCHFourier} most off-diagonal elements vanishes because for the element to be not-null the following relation must stand $\mf{k} = \mf{k'} + \mf{G}$.

Then the continuous-indexed $\mf{H}^{KS}$ matrix becomes a diagonal discrete-indexed block matrix, meaning that one can diagonalize only the non-null blocks on the diagonal, and, more importantly, the operation can be performed independently for each block (i.e. in parallel!).

For computational purposes the discrete index is obviously a great advantage, but we still have to address the problem that we need an infinite number of $G$ to completely describe the system.
Following the same process of the Hartree-Fock method, we select only a subset of all the possible plane waves by imposing a condition on the energy of the plane wave, the so-called \textit{cutoff energy}:
\begin{align}
	\mf{k'}(\erre) = e^{i(\mf{k + G}) \cdot \erre}\\
	\frac{\hbar^2}{2m_e}\mid \mf{k + G} \mid < E_{cut}.
\end{align}

In order to pick the correct cutoff energy a series of tests can be performed with little (and constant) computational effort.

We can now summarize the advantages of plane waves basis sets in periodic potentials:
\begin{enumerate}
	\item It is possible to perform terms calculation in the reciprocal space. Remember that to a differentiation in direct space corresponds a multiplication in reciprocal space! This greatly simplify the calculation on kinetic terms, and often also of $E_{xc}$.\footnote{As said before, the functional is often expressed in function of $(\dens,\grad\dens,\nabla^2\dens)$.}
	\item The continuous indexed matrix $\mf{H}^{KS}$ becomes a finite discrete-indexed matrix.
	\item The $\mf{H}^{KS}$ matrix becomes diagonal at block, and each block can be diagonalized in parallel. \label{en:Kpoints}
\end{enumerate}

Point \ref{en:Kpoints} is the so-called \textit{K-points parallelization}, and, as one can image, it has a great impact on the resolution of the electronic problem in symmetric crystals.

In this work we mainly focused on the study of systems having little symmetries\footnote{Non-periodic systems usually has peculiar physical properties, this is why they are of great research interest and why we focused on them.}, this implies the enlargement of the elementary cell to accommodate all asymmetric factors. 
As a result the $\mf{H}^{KS}$ matrix becomes occupied mostly by a single block and no k-points parallelization can be performed.
This configuration is usually referred as \textit{Lambda point}.


\newpage








We shall now proceed with an in-depth analysis of the implementation of DFT SCF methods in the Quantum ESPRESSO suite, one of the most popular and solid software for materials modeling.







%----------------------------------------------------------------------------
\section{Quantum Espresso}\label{sec:QE}
%----------------------------------------------------------------------------

Quantum ESPRESSO is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale.
It is based on density-functional theory, plane waves, and pseudopotentials.

\mynotes{Scritto per girare su un gazilione di architetture}








\section{Computational Architectures} \label{comparch:sec}

Nowadays the great majority of computational physics is performed on parallel computer architectures; large computing system composed by a multitude of CPUs interacting between each others.

In any parallel computer system, CPUs working on different parts of the same job must communicate to exchange information. Precisely how they should do this is the subject of much debate in the architectural community. 
Two distinct designs, multiprocessors and multicomputers, have been proposed and implemented.

The key difference between the two is the presence or absence of shared memory. 
This difference permeates how they are designed, built, and programmed, as well as their scale and price\cite[p.586]{Tanenbaum}.

\subsection{Multiprocessor Architectures}
A parallel computer in which all the CPUs share a common memory is called a multiprocessor. 
All processes working together on a multiprocessor can share a single virtual address space mapped onto the common memory. 
Any process can read or write a word of memory by just executing a LOAD or STORE instruction.
Two processes can communicate by simply having one of them write data to memory and having the other one read them back.

Because all CPUs in a multiprocessor see the same memory image, there is only one copy of the operating system. 
Consequently, there is only one page map and one process table. 
It is this single-system image that distinguishes a multiprocessor from a multicomputer, in which each computer has its own copy of the operating system.

\begin{comment}
A multiprocessor, like all computers, must have I/O devices, such as disks, network adapters, and other equipment. In some multiprocessor systems, only certain
CPUs have access to the I/O devices, and thus have a special I/O function. In other
ones, every CPU has equal access to every I/O device. When every CPU has equal
access to all the memory modules and all the I/O devices, and is treated as interchangeable with the others by the operating system, the system is called an SMP
(Symmetric MultiProcessor).
\end{comment}

\subsection{Multicomputer Architectures}

\subsection{IBM NetXScale ``Galileo" Cluster Architecture}\label{galileoarch:sec}
\subsection{SGI ALTIX UV-2000 NUMA-CC}\label{numaarch:sec}


\section{Discussion and Conclusion}

May the force be with you.

%In this work, we relate the insect concentration fluctuations to the
%dissipation of Milan University budget to catering companies.
%%
%We find it appropriate to name the present result ``{\em
%fluctuation-dissipation theorem}''.


%------------------------------------------------------------------
%  BIBLIOGRAPHY
%------------------------------------------------------------------
\clearpage
\addcontentsline{toc}{section}{Bibliography}
\begin{thebibliography}{9}

% note that the references must be listed in the same order as they are cited
% in the text above:

%\bibitem{Manini82} % standard format for a book:
%X. Manini and G. D'Annunzio,
%{\it Terra Vergine} (Springer-Verlag, Berlin, 1882).
%
%\bibitem{Jones55} % standard format for a journal article:
%M. Jones and J. Mones, J. Irreprod. Results {\bf 83}, 2322 (1955).
%
%\bibitem{Dragoni11}  % standard format for a thesis:
%D. Dragoni, {\it Interfacial layering of ionic liquids on solid surfaces},
%diploma thesis (University Milan, 2011),
%\url{http://www.mi.infm.it/manini/theses/dragoni.pdf}.

\bibitem{Atkins}
P. W. Atkins and R. S. Friedman,
Molecular Quantum Mechanics,
Oxford University Press, New York,
3rd Edition,
1997.

\bibitem{Attila}
A. Szabo, N. S. Ostlund,
Modern Quantum Chemistry: Introduction to Advanced Electronic Structure Theory,
Dover Pubblications, New York,
1996.

\bibitem{Dan}
D. Dan, Notes on General Chemistry,
Chapter 3.5, Many-electron atoms: Fermi holes and Fermi heaps,
W. H. Freeman Publisher,
2006.

\bibitem{Sakurai}
J. J. Sakurai,
Modern Quantum Mechanics,
Addison-Wesley,
Revised Edition,
1994.

\bibitem{Carati}
A. Carati, L.Galgani,
Appunti di Meccanica Razionale 1,
\url{http://users.mat.unimi.it/users/carati/#Didattica}.

\bibitem{Parr}
R. G. Parr, W. Yang,
Density Functional Theory of Atoms and Molecules,
Oxford University Press,
1989.

\bibitem{Basdevant}
J. L. Basdevant, J. Dalibard,
Quantum Mechanics,
Springer,
2005.


\bibitem{Thomas27}
L. H. Thomas, ``The calculation of atomic fields", Proc. Cambridge Phil. Soc. \textbf{23},542 (1927)

\bibitem{Fermi27}
E. Fermi,``Un Metodo Statistico per la Determinazione di alcune Priopriet\'a dell'Atomo", Rend. Lincei \textbf{6}, 602 (1927)

\bibitem{HK}
P. Hohenberg and W. Kohn, ``Inhomogeneous Electron Gas",  Physical Review \textbf{136} (3B): B864 (1964)

\bibitem{KS}
W. Kohn and L. J. Sham, ``Self-Consistent Equations Including Exchange and Correlation Effects". Physical Review \textbf{140} (4A): A1133 (1965)


\bibitem{Tanenbaum}
A. S. Tanenbaum, T. Austin,
Structured Computer Organization,
Sixth edition,
Pearson,
2012


\bibitem{Martin}
R. Martin, 
Electronic Structure,
Cambridge Press,
2008.

\bibitem{Manini}
N. Manini, 
Introduction to the Physics of Matter,
2006,
\url{http://materia.fisica.unimi.it/manini/dida/Struttura_della_Materia_1.html}.

\end{thebibliography}


\end{document}


